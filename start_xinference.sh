#!/usr/bin/env bash
set -euo pipefail

# Xinference(vLLM) ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ (A100 80G x4 / RAM 930G / CPU 64ì½”ì–´ ìµœì í™”)

MODEL_NAME=${MODEL_NAME:-qwen2.5-vl-instruct}
MODEL_SIZE_B=${MODEL_SIZE_B:-7}
PORT=${PORT:-9997}

# vLLM ê¶Œì¥ ë™ì‹œì„±/ë©”ëª¨ë¦¬ ì„¤ì • (í•„ìš”ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ)
TP_SIZE=${TP_SIZE:-4}                    # 4Ã—A100 ì‚¬ìš©
MAX_MODEL_LEN=${MAX_MODEL_LEN:-8192}
MAX_NUM_SEQS=${MAX_NUM_SEQS:-32}
GPU_MEM_UTIL=${GPU_MEM_UTIL:-0.92}

echo "ğŸš€ Xinference ì„œë²„ ì‹œì‘ ì¤‘..."
echo "ëª¨ë¸: ${MODEL_NAME} (${MODEL_SIZE_B}B)"
echo "ì—”ì§„: vLLM"
echo "í¬íŠ¸: ${PORT}"
echo "í…ì„œ ë³‘ë ¬: ${TP_SIZE}, ì»¨í…ìŠ¤íŠ¸: ${MAX_MODEL_LEN}, ë™ì‹œ ì‹œí€€ìŠ¤: ${MAX_NUM_SEQS}, GPU ë©”ëª¨ë¦¬ ë¹„ìœ¨: ${GPU_MEM_UTIL}"
echo

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1,2,3}
export HF_HOME=${HF_HOME:-/workspace/.hf}
export PYTORCH_CUDA_ALLOC_CONF=${PYTORCH_CUDA_ALLOC_CONF:-expandable_segments:True}
export NCCL_ASYNC_ERROR_HANDLING=${NCCL_ASYNC_ERROR_HANDLING:-1}
export NCCL_DEBUG=${NCCL_DEBUG:-WARN}
export OMP_NUM_THREADS=${OMP_NUM_THREADS:-8}
export MKL_NUM_THREADS=${MKL_NUM_THREADS:-8}
export TOKENIZERS_PARALLELISM=${TOKENIZERS_PARALLELISM:-false}
export CUDA_DEVICE_MAX_CONNECTIONS=${CUDA_DEVICE_MAX_CONNECTIONS:-32}
# vLLM ë‹¤ì¤‘í”„ë¡œì„¸ìŠ¤ ë°©ì‹(êµì°© íšŒí”¼ìš©). í•„ìš” ì‹œ ì¡°ì • ê°€ëŠ¥
export VLLM_WORKER_MULTIPROC_METHOD=${VLLM_WORKER_MULTIPROC_METHOD:-forkserver}

# ì‚¬ì „ í™•ì¸
if ! command -v xinference >/dev/null 2>&1; then
    echo "âŒ xinference ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install xinference[all]' í›„ ì¬ì‹œë„í•˜ì„¸ìš”." >&2
    exit 1
fi

if command -v nvidia-smi >/dev/null 2>&1; then
    echo "ğŸ“Ÿ GPU ìš”ì•½:"; nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
fi

# Xinference(vLLM) ì„œë²„ ì‹œì‘
# ì£¼ì˜: ì•„ë˜ vLLM ê´€ë ¨ í”Œë˜ê·¸ëŠ” ì‚¬ìš© ì¤‘ì¸ Xinference ë²„ì „ì— ë”°ë¼ ì¸ìëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#       'xinference launch --help'ë¡œ ì§€ì› ì¸ìë¥¼ í™•ì¸ í›„ í•„ìš” ì‹œ ìˆ˜ì •í•˜ì„¸ìš”.
xinference launch \
    --model-engine vLLM \
    --model-name "${MODEL_NAME}" \
    --size-in-billions "${MODEL_SIZE_B}" \
    --port "${PORT}" \
    --tensor-parallel-size "${TP_SIZE}" \
    --max-model-len "${MAX_MODEL_LEN}" \
    --max-num-seqs "${MAX_NUM_SEQS}" \
    --gpu-memory-utilization "${GPU_MEM_UTIL}"

echo
echo "âœ… Xinference ì„œë²„ê°€ http://localhost:${PORT} ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."
echo "PDF ë³€í™˜ì„ ì‹œì‘í•˜ë ¤ë©´ 'python main.py' ë˜ëŠ” 'python main_parallel.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."