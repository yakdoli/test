"""
Qwen2.5-VL ìµœëŒ€ ìŠ¤ì¼€ì¼ í´ë¼ì´ì–¸íŠ¸
GPU ë¦¬ì†ŒìŠ¤ í™œìš©ë¥  9-13%ì—ì„œ ìµœëŒ€ í™œìš©ë¥ ë¡œ í™•ì¥
"""

import asyncio
import torch
import gc
import os
import psutil
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
from queue import Queue
import multiprocessing

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
from transformers import AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import config


class MaxScaleResourceManager:
    """ìµœëŒ€ ìŠ¤ì¼€ì¼ GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ - í™œìš©ë¥  ê·¹ëŒ€í™” (ë‹¨ì¼ GPU/ë©€í‹° ì›Œì»¤)"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        self.cpu_count = multiprocessing.cpu_count()
        self.device_properties: Dict[int, Dict[str, Any]] = {}
        
        if self.gpu_available:
            for i in range(self.device_count):
                props = torch.cuda.get_device_properties(i)
                self.device_properties[i] = {
                    'name': props.name,
                    'total_memory': props.total_memory,
                    'compute_capability': f"{props.major}.{props.minor}",
                    'multi_processor_count': props.multi_processor_count
                }
    
    def get_max_scale_config(self) -> Dict[str, Any]:
        """ìµœëŒ€ ìŠ¤ì¼€ì¼ ì„¤ì • - GPU í™œìš©ë¥  ê·¹ëŒ€í™”"""
        if not self.gpu_available:
            raise RuntimeError("GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        total_gpu_memory = sum(props['total_memory'] for props in self.device_properties.values())
        total_gpu_memory_gb = total_gpu_memory / (1024**3)
        
        print(f"ğŸ”§ ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ë¶„ì„:")
        print(f"   GPU: {self.device_count}ê°œ")
        print(f"   ì´ GPU ë©”ëª¨ë¦¬: {total_gpu_memory_gb:.1f}GB")
        print(f"   CPU ì½”ì–´: {self.cpu_count}ê°œ")
        
        # GPUë‹¹ ìˆ˜ìš© ê°€ëŠ¥í•œ ì›Œì»¤ ìˆ˜ ê³„ì‚°
        per_gpu_capacity = self._calculate_per_gpu_worker_capacity()
        max_parallel_instances = sum(per_gpu_capacity.values())
        
        # ì „ì²´ ìµœëŒ€ ì›Œì»¤ ìˆ˜ ì œí•œ ì ìš©
        if config.MAX_TOTAL_GPU_WORKERS is not None:
            max_parallel_instances = min(max_parallel_instances, config.MAX_TOTAL_GPU_WORKERS)
        
        config_settings = {
            "device_map": "auto",
            "torch_dtype": torch.bfloat16,
            "max_memory": self._calculate_aggressive_memory_allocation(),
            "low_cpu_mem_usage": False,
            "use_flash_attention_2": True,
            "max_parallel_instances": max_parallel_instances,
            "per_gpu_capacity": per_gpu_capacity,
            "batch_size": max(1, getattr(config, 'PER_WORKER_BATCH_SIZE', 1)),
            "concurrent_requests": max_parallel_instances  # ì›Œì»¤ ìˆ˜ë§Œí¼ ë™ì‹œ ìš”ì²­
        }
        
        print(f"âš¡ ìµœëŒ€ ìŠ¤ì¼€ì¼ ì„¤ì •:")
        print(f"   GPUë‹¹ ì›Œì»¤ ìˆ˜ìš©ë ¥: {per_gpu_capacity}")
        print(f"   ì´ ë³‘ë ¬ ì¸ìŠ¤í„´ìŠ¤(ì›Œì»¤): {max_parallel_instances}ê°œ")
        print(f"   ë°°ì¹˜ í¬ê¸°(ì›Œì»¤ë‹¹): {config_settings['batch_size']}")
        print(f"   ë™ì‹œ ìš”ì²­ ìƒí•œ: {config_settings['concurrent_requests']}")
        
        return config_settings
    
    def _calculate_aggressive_memory_allocation(self) -> Dict[int, str]:
        """ê³µê²©ì  ë©”ëª¨ë¦¬ í• ë‹¹ - 95% í™œìš©"""
        max_memory: Dict[int, str] = {}
        
        for i in range(self.device_count):
            total_memory = self.device_properties[i]['total_memory']
            # 95% ë©”ëª¨ë¦¬ ì‚¬ìš© (ìµœëŒ€ í™œìš©)
            usable_memory = int(total_memory * 0.95)
            max_memory[i] = f"{usable_memory // (1024**3)}GB"
        
        return max_memory
    
    def _calculate_per_gpu_worker_capacity(self) -> Dict[int, int]:
        """GPU ë©”ëª¨ë¦¬/ì„¤ì • ê¸°ë°˜ GPUë‹¹ ì›Œì»¤ ìˆ˜ìš©ë ¥ ê³„ì‚°"""
        capacity: Dict[int, int] = {}
        if not self.gpu_available:
            return capacity
        
        for i, props in self.device_properties.items():
            total_gb = props['total_memory'] / (1024**3)
            usable_gb = total_gb * float(getattr(config, 'GPU_MAX_MEMORY_FRACTION', 0.9))
            est_per_worker = float(getattr(config, 'WORKER_ESTIMATED_VRAM_GB', 16.0))
            # ìµœì†Œ 1ê°œ ë³´ì¥
            max_by_mem = max(1, int(usable_gb // est_per_worker))
            if getattr(config, 'MAX_WORKERS_PER_GPU', None) is not None:
                max_by_mem = min(max_by_mem, int(config.MAX_WORKERS_PER_GPU))
            capacity[i] = max_by_mem
        return capacity
    
    def build_worker_plan(self) -> List[int]:
        """ì „ì²´ ì›Œì»¤ ìˆ˜ì— ëŒ€í•œ GPU í• ë‹¹ ê³„íš ìƒì„± (ë¦¬ìŠ¤íŠ¸: device_idë“¤ì˜ ë‚˜ì—´)"""
        per_gpu_capacity = self._calculate_per_gpu_worker_capacity()
        device_ids: List[int] = []
        # ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ GPUë‹¹ capacityë§Œí¼ ì¶”ê°€
        more = True
        while more:
            more = False
            for gpu_id, cap in per_gpu_capacity.items():
                if sum(1 for d in device_ids if d == gpu_id) < cap:
                    device_ids.append(gpu_id)
                    more = True
                    # ì „ì²´ í•œë„ ì²´í¬
                    if getattr(config, 'MAX_TOTAL_GPU_WORKERS', None) is not None and len(device_ids) >= int(config.MAX_TOTAL_GPU_WORKERS):
                        return device_ids
        return device_ids


class MaxScaleQwenWorker:
    """ìµœëŒ€ ìŠ¤ì¼€ì¼ ì›Œì»¤ - ë‹¨ì¼ GPU ì¸ìŠ¤í„´ìŠ¤ (GPUë³„ ì§ê²°, ë‚´ë¶€ ë§ˆì´í¬ë¡œë°°ì¹˜ ë³‘ë ¬)"""
    
    def __init__(self, worker_id: int, device_id: int, executor: ThreadPoolExecutor, tokenizer, processor):
        self.worker_id = worker_id
        self.device_id = device_id
        self.model = None
        self.tokenizer = None
        self.processor = None
        self._lock = asyncio.Lock()  # generate í˜¸ì¶œ ì¤‘ë³µ ë°©ì§€
        self.max_concurrency = max(1, int(getattr(config, 'PER_WORKER_CONCURRENCY', 1)))
        self.stats = {
            'processed_requests': 0,
            'total_time': 0,
            'device_utilization': []
        }
        self.executor = executor
        self.tokenizer = tokenizer
        self.processor = processor
        
    async def initialize(self) -> bool:
        """ì›Œì»¤ ì´ˆê¸°í™”"""
        try:
            print(f"ğŸ”§ ì›Œì»¤ {self.worker_id} (GPU {self.device_id}) ì´ˆê¸°í™” ì¤‘...")
            
            # ëª¨ë¸ ë¡œë”©ì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(self.executor, self._load_model_sync)
            
            print(f"âœ… ì›Œì»¤ {self.worker_id} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì›Œì»¤ {self.worker_id} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _load_model_sync(self):
        """ëª¨ë¸ ë¡œë”© (ë™ê¸°) - ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰"""
        # Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        os.environ["HF_HOME"] = config.HF_CACHE_DIR
        
        load_kwargs = {
            "pretrained_model_name_or_path": config.QWEN_MODEL_PATH,
            "trust_remote_code": config.QWEN_TRUST_REMOTE_CODE,
            "device_map": f"cuda:{self.device_id}",
            "torch_dtype": "auto",
            "attn_implementation": "flash_attention_2",
            "low_cpu_mem_usage": False, # ì›Œì»¤ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œ ë©”íƒ€ í…ì„œ ì˜¤ë¥˜ ë°©ì§€
        }
        
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(**load_kwargs)
        self.model.to(f"cuda:{self.device_id}") # ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ GPUë¡œ ì´ë™
        self.model.eval()
    
    async def process_image_batch(self, image_batch: List[Path]) -> List[str]:
        """ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬ - ì›Œì»¤ ë‚´ë¶€ ë§ˆì´í¬ë¡œë°°ì¹˜/ë™ì‹œ ì²˜ë¦¬ ì§€ì›"""
        async with self._lock:  # generate í˜¸ì¶œ ì¤‘ë³µ ë°©ì§€ (ëª¨ë¸ ê³µìœ )
            start_time = datetime.now()
            results: List[str] = [None] * len(image_batch)
            
            try:
                # ë§ˆì´í¬ë¡œë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ”
                micro = max(1, self.max_concurrency)
                for i in range(0, len(image_batch), micro):
                    chunk_paths = image_batch[i:i+micro]
                    # ë™ì‹œ ì „ì²˜ë¦¬ + ê°œë³„ generateë¥¼ íƒœìŠ¤í¬ë¡œ ì˜¬ë ¤ë„, ê°™ì€ ëª¨ë¸ ê³µìœ ë¼ ì‹¤ì œë¡œëŠ” ìˆœì°¨ generateê°€ ì•ˆì „í•¨
                    # ëŒ€ì‹  ì—¬ê¸°ì„œëŠ” ì „ì²˜ë¦¬ë¥¼ ë³‘ë ¬í™”í•˜ê³ , generateëŠ” ìˆœì°¨ë¡œ í•©ì³ ì†Œí­ concurrencyë¥¼ ì œê³µ
                    preprocess_tasks = [asyncio.create_task(self._prepare_inputs(p)) for p in chunk_paths]
                    prepared = await asyncio.gather(*preprocess_tasks, return_exceptions=True)
                    
                    # ìˆœì°¨ generate (VRAM ìŠ¤íŒŒì´í¬ ë°©ì§€)
                    for idx_in, prep in enumerate(prepared):
                        j = i + idx_in
                        if isinstance(prep, Exception) or prep is None:
                            results[j] = f"<!-- ì „ì²˜ë¦¬ ì‹¤íŒ¨: {chunk_paths[idx_in].name} -->"
                            continue
                        markdown = await self._generate_from_prepared(*prep)
                        results[j] = markdown if markdown else f"<!-- ì²˜ë¦¬ ì‹¤íŒ¨: {chunk_paths[idx_in].name} -->"
                        self.stats['processed_requests'] += 1
                
                processing_time = (datetime.now() - start_time).total_seconds()
                self.stats['total_time'] += processing_time
                
                # GPU í™œìš©ë¥  ê¸°ë¡ (ê°€ìš© ì‹œ)
                try:
                    gpu_util = torch.cuda.utilization(self.device_id)
                    self.stats['device_utilization'].append(gpu_util)
                except Exception:
                    pass
                
                return results
                
            except Exception as e:
                print(f"âŒ ì›Œì»¤ {self.worker_id} ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                return [f"<!-- ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e} -->" for _ in image_batch]
    
    async def _process_single_image(self, image_path: Path) -> Optional[str]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ (ë ˆê±°ì‹œ ê²½ë¡œ)"""
        try:
            prep = await self._prepare_inputs(image_path)
            if prep is None:
                return None
            return await self._generate_from_prepared(*prep)
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({image_path.name}): {e}")
            return None

    async def _prepare_inputs(self, image_path: Path):
        """ì „ì²˜ë¦¬/ì…ë ¥ ì¤€ë¹„ (ë³‘ë ¬ ê°€ëŠ¥)"""
        try:
            loop = asyncio.get_running_loop()
            # CPU ë°”ìš´ë“œ ì „ì²˜ë¦¬ ì‘ì—…ì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
            inputs_data = await loop.run_in_executor(
                self.executor,
                self._prepare_inputs_sync, # ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ
                image_path
            )
            
            # GPUë¡œ ì´ë™ (ë¹„ë™ê¸°)
            inputs = {k: v.to(f"cuda:{self.device_id}") if hasattr(v, 'to') else v 
                      for k, v in inputs_data.items()}
            return inputs, image_path
        except Exception as e:
            print(f"âš  ì „ì²˜ë¦¬ ì‹¤íŒ¨ ({image_path.name}): {e}")
            return None

    def _prepare_inputs_sync(self, image_path: Path):
        """ì „ì²˜ë¦¬/ì…ë ¥ ì¤€ë¹„ (ë™ê¸°) - ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰"""
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": str(image_path)},
                {"type": "text", "text": self._get_optimized_prompt()}
            ]
        }]
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        return inputs

    async def _generate_from_prepared(self, inputs, image_path: Path) -> Optional[str]:
        """ì¤€ë¹„ëœ ì…ë ¥ìœ¼ë¡œë¶€í„° generate ìˆ˜í–‰ (ìˆœì°¨/ë½ í•˜ì— í˜¸ì¶œë¨)"""
        try:
            generation_config = {
                "max_new_tokens": 8192,
                "do_sample": False,
                "top_p": 0.9,
                "use_cache": True,
                "repetition_penalty": 1.05,
                "pad_token_id": self.tokenizer.eos_token_id,
            }
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, **generation_config)
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            torch.cuda.empty_cache()
            return output_text
        except Exception as e:
            print(f"âŒ generate ì‹¤íŒ¨ ({image_path.name}): {e}")
            return None
    
    def _get_optimized_prompt(self) -> str:
        """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ (ê°„ê²°í•˜ì§€ë§Œ íš¨ê³¼ì )"""
        return """Convert this technical documentation image to structured markdown format.

Requirements:
- Extract ALL code snippets with proper language tags (```csharp, ```xml, etc.)
- Preserve exact syntax and formatting
- Use clear heading hierarchy (# ## ###)
- Create parameter tables: Name | Type | Description
- Include all visible text and technical details
- Format examples with "Example:" headers
- Maintain numbered/bulleted lists
- Add metadata for categorization

Focus on accuracy and completeness for LLM training data."""
    
    def cleanup(self):
        """ì›Œì»¤ ì •ë¦¬"""
        if self.model:
            del self.model
            self.model = None
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        if self.processor:
            del self.processor
            self.processor = None
        
        torch.cuda.empty_cache()


from typing import List, Optional, Dict, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading
from queue import Queue
import multiprocessing

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
from transformers import AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import config


class MaxScaleQwenClient:
    """ìµœëŒ€ ìŠ¤ì¼€ì¼ Qwen í´ë¼ì´ì–¸íŠ¸ - GPU í™œìš©ë¥  ê·¹ëŒ€í™” (ë‹¨ì¼ GPUë‹¹ ë‹¤ì¤‘ ì›Œì»¤)"""
    
    def __init__(self):
        self.resource_manager = MaxScaleResourceManager()
        self.workers: List[MaxScaleQwenWorker] = []
        self.config: Optional[Dict[str, Any]] = None
        self.work_queue = Queue()
        self.result_queue = Queue()
        self.executor = ThreadPoolExecutor() # CPU ë°”ìš´ë“œ ì‘ì—…ì„ ìœ„í•œ ìŠ¤ë ˆë“œ í’€
        self.stats = {
            'total_images_processed': 0,
            'total_processing_time': 0,
            'peak_gpu_utilization': {},
            'throughput_stats': []
        }

    async def _preload_model_to_cache(self):
        """ëª¨ë¸ì„ ë¡œì»¬ ìºì‹œì— ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ë‹¨ì¼ ìŠ¤ë ˆë“œ)"""
        print("ğŸš€ ëª¨ë¸ì„ ë¡œì»¬ ìºì‹œì— ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        os.environ["HF_HOME"] = config.HF_CACHE_DIR
        
        # í† í¬ë‚˜ì´ì €ì™€ í”„ë¡œì„¸ì„œ ë¡œë“œ (ë‹¤ìš´ë¡œë“œ íŠ¸ë¦¬ê±°)
        self.preloaded_tokenizer = AutoTokenizer.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        self.preloaded_processor = AutoProcessor.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        
        # ëª¨ë¸ ë¡œë“œ (ë‹¤ìš´ë¡œë“œ íŠ¸ë¦¬ê±°)
        load_kwargs = {
            "pretrained_model_name_or_path": config.QWEN_MODEL_PATH,
            "trust_remote_code": config.QWEN_TRUST_REMOTE_CODE,
            "torch_dtype": "auto",
            "attn_implementation": "flash_attention_2" if config.QWEN_USE_FLASH_ATTENTION else "eager",
            "low_cpu_mem_usage": False,
            "use_safetensors": True,
        }
        Qwen2_5_VLForConditionalGeneration.from_pretrained(**load_kwargs)
        print("âœ… ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")

    async def initialize_max_scale_system(self) -> bool:
        """ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸš€ ìµœëŒ€ ìŠ¤ì¼€ì¼ Qwen ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (ë‹¨ì¼ ìŠ¤ë ˆë“œ)
            await self._preload_model_to_cache()

            self.config = self.resource_manager.get_max_scale_config()
            
            # ì›Œì»¤ ìƒì„± ë° ì´ˆê¸°í™”
            max_workers = self.config['max_parallel_instances']
            per_gpu_capacity: Dict[int, int] = self.config.get('per_gpu_capacity', {})
            device_plan = self.resource_manager.build_worker_plan() if not per_gpu_capacity else [d for g, cap in per_gpu_capacity.items() for d in [g]*cap]
            if config.MAX_TOTAL_GPU_WORKERS is not None:
                device_plan = device_plan[:config.MAX_TOTAL_GPU_WORKERS]
            
            print(f"ğŸ‘¥ {len(device_plan)}ê°œ ì›Œì»¤ ìƒì„± ì¤‘ (ê³„íš: {device_plan})...")
            
            # ê³„íšì— ë”°ë¼ GPUì— ì›Œì»¤ ë¶„ë°° (ê° ì›Œì»¤ëŠ” ë‹¨ì¼ GPU ì „ìš©)
            worker_init_tasks = []
            temp_workers = []
            for i, device_id in enumerate(device_plan):
                worker = MaxScaleQwenWorker(worker_id=i, device_id=device_id, executor=self.executor, 
                                            tokenizer=self.preloaded_tokenizer, processor=self.preloaded_processor)
                temp_workers.append(worker)
                worker_init_tasks.append(worker.initialize())

            # ëª¨ë“  ì›Œì»¤ ì´ˆê¸°í™”ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
            init_results = await asyncio.gather(*worker_init_tasks, return_exceptions=True)

            for i, result in enumerate(init_results):
                if isinstance(result, Exception):
                    print(f"âŒ ì›Œì»¤ {temp_workers[i].worker_id} (GPU {temp_workers[i].device_id}) ì´ˆê¸°í™” ì‹¤íŒ¨: {result}")
                elif result:
                    self.workers.append(temp_workers[i])
                    print(f"âœ… ì›Œì»¤ {temp_workers[i].worker_id} (GPU {temp_workers[i].device_id}) ì¤€ë¹„ ì™„ë£Œ")
                else:
                    print(f"âŒ ì›Œì»¤ {temp_workers[i].worker_id} (GPU {temp_workers[i].device_id}) ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            if not self.workers:
                print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì›Œì»¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print(f"ğŸ¯ ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ: {len(self.workers)}ê°œ ì›Œì»¤")
            return True
            
        except Exception as e:
            print(f"âŒ ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def convert_images_max_scale(self, image_paths: List[Path]) -> str:
        """ìµœëŒ€ ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ ë³€í™˜ - ìµœëŒ€ GPU í™œìš©ë¥ """
        if not self.workers:
            if not await self.initialize_max_scale_system():
                return "ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨"
        
        total_images = len(image_paths)
        batch_size = self.config['batch_size']
        
        print(f"ğŸš€ ìµœëŒ€ ìŠ¤ì¼€ì¼ ë³€í™˜ ì‹œì‘:")
        print(f"   ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"   ì›Œì»¤ ìˆ˜: {len(self.workers)}ê°œ")
        print(f"   ë°°ì¹˜ í¬ê¸°(ì›Œì»¤ë‹¹): {batch_size}")
        print(f"   ì˜ˆìƒ ë™ì‹œ ì²˜ë¦¬ëŸ‰: {len(self.workers) * batch_size}ê°œ")
        
        start_time = datetime.now()
        
        # ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
        image_batches = [
            image_paths[i:i + batch_size] 
            for i in range(0, total_images, batch_size)
        ]
        
        print(f"ğŸ“¦ {len(image_batches)}ê°œ ë°°ì¹˜ ìƒì„±")
        
        # ì‘ì—… í ìƒì„± ë° ë°°ì¹˜ ì¶”ê°€
        task_queue = asyncio.Queue()
        for batch in image_batches:
            await task_queue.put(batch)
        
        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ ì¸ë±ìŠ¤ ì‚¬ìš©)
        results_map: Dict[int, List[str]] = {}
        results_lock = asyncio.Lock() # ê²°ê³¼ ë§µ ì ‘ê·¼ ë³´í˜¸
        
        # ì›Œì»¤ ì½”ë£¨í‹´ ì •ì˜
        async def worker_task(worker: MaxScaleQwenWorker):
            nonlocal results_map, results_lock
            while True:
                batch_to_process = await task_queue.get()
                if batch_to_process is None: # Sentinel value to stop worker
                    task_queue.task_done()
                    break
                
                try:
                    # ë°°ì¹˜ ì²˜ë¦¬
                    batch_results = await worker.process_image_batch(batch_to_process)
                    
                    # ê²°ê³¼ ë§µì— ì €ì¥ (ì›ë˜ ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´)
                    async with results_lock:
                        # Find the original index of the first image in the batch
                        first_image_path = batch_to_process[0]
                        original_index = image_paths.index(first_image_path)
                        results_map[original_index] = batch_results
                        
                except Exception as e:
                    print(f"âŒ ì›Œì»¤ {worker.worker_id} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    async with results_lock:
                        first_image_path = batch_to_process[0]
                        original_index = image_paths.index(first_image_path)
                        results_map[original_index] = [f"<!-- ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)} -->" for _ in batch_to_process]
                finally:
                    task_queue.task_done()

        # ëª¨ë“  ì›Œì»¤ì— ëŒ€í•œ íƒœìŠ¤í¬ ìƒì„±
        worker_coroutines = [worker_task(worker) for worker in self.workers]
        
        # ëª¨ë“  ì›Œì»¤ê°€ ì‘ì—…ì„ ë§ˆì¹  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        await asyncio.gather(*worker_coroutines)
        
        # ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŒì„ íì— ì•Œë¦¼ (ì›Œì»¤ ì¢…ë£Œìš©)
        for _ in self.workers:
            await task_queue.put(None)
        
        await task_queue.join() # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        
        # ê²°ê³¼ ì¡°í•© (ì›ë˜ ì´ë¯¸ì§€ ìˆœì„œëŒ€ë¡œ)
        markdown_content = []
        page_counter = 1
        
        # results_mapì˜ í‚¤(original_index)ë¥¼ ì •ë ¬í•˜ì—¬ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        sorted_indices = sorted(results_map.keys())
        
        for original_index in sorted_indices:
            batch_results = results_map[original_index]
            for result in batch_results:
                if page_counter > 1:
                    markdown_content.append("\n---\n")
                markdown_content.append(f"<!-- í˜ì´ì§€ {page_counter} -->\n")
                markdown_content.append(result)
                page_counter += 1
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        # ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘
        self._collect_performance_stats(total_time, total_images)
        
        print(f"\nğŸ“Š ìµœëŒ€ ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"  ğŸš€ ì²˜ë¦¬ëŸ‰: {total_images / total_time:.2f} í˜ì´ì§€/ì´ˆ")
        print(f"  ğŸ“ˆ ì›Œì»¤ë‹¹ í‰ê· : {total_images / len(self.workers) / total_time:.2f} í˜ì´ì§€/ì´ˆ")
        
        # GPU í™œìš©ë¥  í†µê³„
        self._print_gpu_utilization_stats()
        
        return "\n".join(markdown_content)
    
    def _collect_performance_stats(self, total_time: float, total_images: int):
        """ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘"""
        self.stats['total_images_processed'] = total_images
        self.stats['total_processing_time'] = total_time
        self.stats['throughput_stats'].append({
            'images': total_images,
            'time': total_time,
            'throughput': total_images / total_time,
            'workers': len(self.workers)
        })
        
        # ì›Œì»¤ë³„ GPU í™œìš©ë¥  ìˆ˜ì§‘
        for worker in self.workers:
            if worker.stats['device_utilization']:
                device_key = f"gpu_{worker.device_id}"
                max_util = max(worker.stats['device_utilization'])
                avg_util = sum(worker.stats['device_utilization']) / len(worker.stats['device_utilization'])
                
                if device_key not in self.stats['peak_gpu_utilization']:
                    self.stats['peak_gpu_utilization'][device_key] = {
                        'peak': max_util,
                        'average': avg_util,
                        'workers': []
                    }
                
                self.stats['peak_gpu_utilization'][device_key]['peak'] = max(
                    self.stats['peak_gpu_utilization'][device_key]['peak'], max_util
                )
                self.stats['peak_gpu_utilization'][device_key]['workers'].append(worker.worker_id)
    
    def _print_gpu_utilization_stats(self):
        """GPU í™œìš©ë¥  í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ’¾ GPU í™œìš©ë¥  í†µê³„:")
        for device_key, stats in self.stats['peak_gpu_utilization'].items():
            print(f"  {device_key}:")
            print(f"    í”¼í¬: {stats['peak']:.1f}%")
            print(f"    í‰ê· : {stats['average']:.1f}%")
            print(f"    ì›Œì»¤: {len(stats['workers'])}ê°œ")
    
    def get_max_scale_stats(self) -> Dict[str, Any]:
        """ìµœëŒ€ ìŠ¤ì¼€ì¼ í†µê³„ ë°˜í™˜"""
        return {
            'mode': 'max_scale_optimized',
            'worker_count': len(self.workers),
            'gpu_count': self.resource_manager.device_count,
            'max_parallel_instances': self.config.get('max_parallel_instances', 0) if self.config else 0,
            'batch_size': self.config.get('batch_size', 0) if self.config else 0,
            'performance_stats': self.stats
        }
    
    def cleanup_all_workers(self):
        """ëª¨ë“  ì›Œì»¤ ì •ë¦¬"""
        print("ğŸ§¹ ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        
        for worker in self.workers:
            worker.cleanup()
        
        self.workers.clear()
        
        if self.resource_manager.gpu_available:
            for i in range(self.resource_manager.device_count):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
        
        gc.collect()
        
        if self.executor:
            self.executor.shutdown()
            
        print("âœ… ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")


async def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    client = MaxScaleQwenClient()
    
    print("ğŸ§ª ìµœëŒ€ ìŠ¤ì¼€ì¼ í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸")
    if await client.initialize_max_scale_system():
        print("âœ… ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ")
        
        stats = client.get_max_scale_stats()
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ì„¤ì •:")
        print(f"   ì›Œì»¤: {stats['worker_count']}ê°œ")
        print(f"   GPU: {stats['gpu_count']}ê°œ")
        print(f"   ìµœëŒ€ ë³‘ë ¬: {stats['max_parallel_instances']}")
        
        client.cleanup_all_workers()
    else:
        print("âŒ ìµœëŒ€ ìŠ¤ì¼€ì¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(main())