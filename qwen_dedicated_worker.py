"""
GPU ì „ìš© ì›Œì»¤ ëª¨ë“ˆ - ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰
"""

import torch
import os
import pickle
from pathlib import Path
from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
from transformers import AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import config


def process_image_batch_worker(data):
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜"""
    device_id, image_batch_serialized = data
    
    try:
        # CUDA ë””ë°”ì´ìŠ¤ ì„¤ì •
        os.environ['CUDA_VISIBLE_DEVICES'] = str(device_id)
        torch.cuda.set_device(0)
        
        # ì´ë¯¸ì§€ ë°°ì¹˜ ì—­ì§ë ¬í™”
        image_paths = pickle.loads(image_batch_serialized)
        
        print(f"ğŸ”§ GPU {device_id} ì›Œì»¤ ì´ˆê¸°í™” ì¤‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        
        # í”„ë¡œì„¸ì„œ ë¡œë“œ
        processor = AutoProcessor.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        
        # ëª¨ë¸ ë¡œë“œ (í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ GPUë§Œ ì‚¬ìš©)
        load_kwargs = {
            "pretrained_model_name_or_path": config.QWEN_MODEL_PATH,
            "trust_remote_code": config.QWEN_TRUST_REMOTE_CODE,
            "device_map": "cuda:0",
            "torch_dtype": torch.bfloat16,
            "attn_implementation": "flash_attention_2",
            "low_cpu_mem_usage": True
        }
        
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(**load_kwargs)
        model.eval()
        
        print(f"âœ… GPU {device_id} ì›Œì»¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        results = []
        for image_path in image_paths:
            try:
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                messages = [{
                    "role": "user",
                    "content": [
                        {"type": "image", "image": str(image_path)},
                        {"type": "text", "text": get_optimized_prompt()}
                    ]
                }]
                
                # ì „ì²˜ë¦¬
                text = processor.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                
                image_inputs, video_inputs = process_vision_info(messages)
                
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt"
                )
                
                # GPUë¡œ ì´ë™
                inputs = {k: v.to("cuda:0") if hasattr(v, 'to') else v 
                         for k, v in inputs.items()}
                
                # ì…ë ¥ ê²€ì¦
                if 'input_ids' not in inputs:
                    raise ValueError("input_ids not found in inputs")
                
                # ìƒì„± ì„¤ì •
                generation_config = {
                    "max_new_tokens": 2500,
                    "do_sample": False,
                    "temperature": 0.1,
                    "pad_token_id": tokenizer.eos_token_id,
                    "use_cache": True,
                }
                
                # í…ìŠ¤íŠ¸ ìƒì„±
                with torch.no_grad():
                    generated_ids = model.generate(**inputs, **generation_config)
                
                # ì‘ë‹µ ì¶”ì¶œ
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
                ]
                
                output_text = processor.batch_decode(
                    generated_ids_trimmed, 
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False
                )[0]
                
                results.append(output_text)
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"âŒ GPU {device_id} ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({image_path}): {e}")
                results.append(f"<!-- GPU {device_id} ì²˜ë¦¬ ì‹¤íŒ¨: {image_path.name} -->")
        
        print(f"âœ… GPU {device_id} ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({len(results)}ê°œ)")
        return results
        
    except Exception as e:
        print(f"âŒ GPU {device_id} ì›Œì»¤ ì˜¤ë¥˜: {e}")
        return [f"<!-- GPU {device_id} ì›Œì»¤ ì‹¤íŒ¨ -->"]


def get_optimized_prompt() -> str:
    """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸"""
    return """Convert this technical documentation to markdown format.

Key requirements:
- Extract code snippets with proper language tags (```csharp, ```xml)
- Use clear heading structure (# ## ###)  
- Include all visible text and technical details
- Format tables and lists properly
- Add code examples with proper formatting

Focus on accuracy and completeness."""