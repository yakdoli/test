"""
GPU ì „ìš© ì›Œì»¤ ì‹œìŠ¤í…œ - ê° ì›Œì»¤ëŠ” ë‹¨ì¼ GPU ì „ìš© ì‚¬ìš©
GPUê°„ ì˜¤í”„ë¡œë“œ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”ë¥¼ ìœ„í•œ ìµœì í™”
"""

import asyncio
import torch
import gc
import os
import psutil
import multiprocessing
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor
import pickle

# CUDA ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ìœ„í•œ spawn ë°©ì‹ ì„¤ì •
multiprocessing.set_start_method('spawn', force=True)

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
from transformers import AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import config


class DedicatedGPUResourceManager:
    """GPU ì „ìš© ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì - ì˜¤í”„ë¡œë“œ ìµœì†Œí™”"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        self.cpu_count = multiprocessing.cpu_count()
        self.device_properties = {}
        
        if self.gpu_available:
            for i in range(self.device_count):
                props = torch.cuda.get_device_properties(i)
                self.device_properties[i] = {
                    'name': props.name,
                    'total_memory': props.total_memory,
                    'compute_capability': f"{props.major}.{props.minor}",
                }
    
    def get_dedicated_gpu_config(self, device_id: int) -> Dict[str, Any]:
        """íŠ¹ì • GPU ì „ìš© ì„¤ì • ë°˜í™˜"""
        if not self.gpu_available or device_id >= self.device_count:
            raise ValueError(f"GPU {device_id}ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        device_memory = self.device_properties[device_id]['total_memory']
        device_memory_gb = device_memory / (1024**3)
        
        print(f"ğŸ”§ GPU {device_id} ì „ìš© ì„¤ì •:")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device_properties[device_id]['name']}")
        print(f"   ë©”ëª¨ë¦¬: {device_memory_gb:.1f}GB")
        
        # ë‹¨ì¼ GPU ì „ìš© ìµœì í™” ì„¤ì •
        config_settings = {
            "device_map": f"cuda:{device_id}",  # íŠ¹ì • GPUë§Œ ì‚¬ìš©
            "torch_dtype": torch.bfloat16,
            "use_flash_attention_2": True,
            "low_cpu_mem_usage": True,
            # GPUê°„ ì˜¤í”„ë¡œë“œ ì™„ì „ ì°¨ë‹¨
            "offload_folder": None,
            "offload_state_dict": False,
            # ë©”ëª¨ë¦¬ ìµœì í™” (95% ì‚¬ìš©)
            "max_memory": {device_id: f"{int(device_memory_gb * 0.95)}GB"}
        }
        
        return config_settings


def initialize_dedicated_worker(device_id: int, worker_config: Dict[str, Any]) -> bool:
    """í”„ë¡œì„¸ìŠ¤ë³„ GPU ì „ìš© ì›Œì»¤ ì´ˆê¸°í™”"""
    try:
        # CUDA ë””ë°”ì´ìŠ¤ ì„¤ì •
        os.environ['CUDA_VISIBLE_DEVICES'] = str(device_id)
        torch.cuda.set_device(0)  # ë³´ì´ëŠ” ì²« ë²ˆì§¸ ë””ë°”ì´ìŠ¤ ì‚¬ìš©
        
        print(f"ğŸ”§ í”„ë¡œì„¸ìŠ¤ë³„ ì›Œì»¤ {device_id} ì´ˆê¸°í™” ì¤‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        
        # í”„ë¡œì„¸ì„œ ë¡œë“œ
        processor = AutoProcessor.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        
        # ëª¨ë¸ ë¡œë“œ (ë‹¨ì¼ GPU ì „ìš©)
        load_kwargs = {
            "pretrained_model_name_or_path": config.QWEN_MODEL_PATH,
            "trust_remote_code": config.QWEN_TRUST_REMOTE_CODE,
            "device_map": "cuda:0",  # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ìœ ì¼í•œ GPU
            "torch_dtype": torch.bfloat16,
            "attn_implementation": "flash_attention_2",
            "low_cpu_mem_usage": True
        }
        
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(**load_kwargs)
        model.eval()
        
        # ê¸€ë¡œë²Œ ë³€ìˆ˜ë¡œ ì €ì¥
        globals()['model'] = model
        globals()['tokenizer'] = tokenizer
        globals()['processor'] = processor
        globals()['device_id'] = device_id
        
        print(f"âœ… í”„ë¡œì„¸ìŠ¤ë³„ ì›Œì»¤ {device_id} ì´ˆê¸°í™” ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ í”„ë¡œì„¸ìŠ¤ë³„ ì›Œì»¤ {device_id} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False


def process_image_batch_dedicated(image_batch_data: bytes) -> List[str]:
    """GPU ì „ìš© ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        # ì§ë ¬í™”ëœ ë°ì´í„° ë³µì›
        image_paths = pickle.loads(image_batch_data)
        
        model = globals()['model']
        tokenizer = globals()['tokenizer'] 
        processor = globals()['processor']
        device_id = globals()['device_id']
        
        results = []
        
        for image_path in image_paths:
            try:
                # Syncfusion íŠ¹í™” í”„ë¡¬í”„íŠ¸ (ê°„ì†Œí™”)
                messages = [{
                    "role": "user",
                    "content": [
                        {"type": "image", "image": str(image_path)},
                        {"type": "text", "text": get_optimized_prompt()}
                    ]
                }]
                
                # ì „ì²˜ë¦¬
                text = processor.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                
                image_inputs, video_inputs = process_vision_info(messages)
                
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt"
                )
                
                # í˜„ì¬ GPUë¡œ ì´ë™ (ì˜¤í”„ë¡œë“œ ì—†ìŒ)
                inputs = {k: v.to("cuda:0") if hasattr(v, 'to') else v 
                         for k, v in inputs.items()}
                
                # ìµœì í™”ëœ ìƒì„± ì„¤ì •
                generation_config = {
                    "max_new_tokens": 2500,
                    "do_sample": False,
                    "temperature": 0.1,
                    "pad_token_id": tokenizer.eos_token_id,
                    "use_cache": True,
                }
                
                # í…ìŠ¤íŠ¸ ìƒì„± (ë‹¨ì¼ GPUì—ì„œë§Œ)
                with torch.no_grad():
                    generated_ids = model.generate(**inputs, **generation_config)
                
                # ì‘ë‹µ ì¶”ì¶œ
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                output_text = processor.batch_decode(
                    generated_ids_trimmed, 
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False
                )[0]
                
                results.append(output_text)
                
                # ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”)
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({image_path}): {e}")
                results.append(f"<!-- GPU {device_id} ì²˜ë¦¬ ì‹¤íŒ¨: {image_path.name} -->")
        
        return results
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return [f"<!-- ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e} -->"]


def get_optimized_prompt() -> str:
    """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ (ë¹ ë¥¸ ì²˜ë¦¬ìš©)"""
    return """Convert this technical documentation to markdown format.

Key requirements:
- Extract code snippets with proper language tags (```csharp, ```xml)
- Use clear heading structure (# ## ###)
- Include all visible text and technical details
- Format tables and lists properly
- Add code examples with proper formatting

Focus on accuracy and completeness."""


class DedicatedGPUQwenClient:
    """GPU ì „ìš© Qwen í´ë¼ì´ì–¸íŠ¸ - ì˜¤í”„ë¡œë“œ ìµœì†Œí™”"""
    
    def __init__(self):
        self.resource_manager = DedicatedGPUResourceManager()
        self.process_pool = None
        self.workers_initialized = False
        self.stats = {
            'total_images_processed': 0,
            'total_processing_time': 0,
            'gpu_utilization_stats': {},
            'processing_times': []
        }
        
    async def initialize_dedicated_system(self) -> bool:
        """GPU ì „ìš© ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸš€ GPU ì „ìš© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        if not self.resource_manager.gpu_available:
            print("âŒ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return False
        
        gpu_count = self.resource_manager.device_count
        print(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpu_count}ê°œ")
        
        try:
            # ê° GPUë³„ í”„ë¡œì„¸ìŠ¤ í’€ ìƒì„± (spawn ë°©ì‹)
            ctx = multiprocessing.get_context('spawn')
            self.process_pool = ProcessPoolExecutor(
                max_workers=gpu_count,
                mp_context=ctx
            )
            
            print(f"âœ… GPU ì „ìš© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {gpu_count}ê°œ í”„ë¡œì„¸ìŠ¤")
            self.workers_initialized = True
            return True
            
        except Exception as e:
            print(f"âŒ GPU ì „ìš© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def convert_images_dedicated_gpu(self, image_paths: List[Path]) -> str:
        """GPU ì „ìš© ì´ë¯¸ì§€ ë³€í™˜"""
        if not self.workers_initialized:
            if not await self.initialize_dedicated_system():
                return "GPU ì „ìš© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨"
        
        total_images = len(image_paths)
        gpu_count = self.resource_manager.device_count
        
        # GPUë³„ ë°°ì¹˜ ë¶„í•  (ê° GPUê°€ ì „ìš©ìœ¼ë¡œ ì²˜ë¦¬)
        images_per_gpu = total_images // gpu_count
        remainder = total_images % gpu_count
        
        batches = []
        start_idx = 0
        
        for i in range(gpu_count):
            batch_size = images_per_gpu + (1 if i < remainder else 0)
            if batch_size > 0:
                batch = image_paths[start_idx:start_idx + batch_size]
                batches.append(batch)
                start_idx += batch_size
        
        print(f"ğŸš€ GPU ì „ìš© ë³€í™˜ ì‹œì‘:")
        print(f"   ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"   GPU ìˆ˜: {gpu_count}ê°œ")
        print(f"   ë°°ì¹˜ ìˆ˜: {len(batches)}ê°œ")
        print(f"   GPUë³„ í‰ê· : {total_images/gpu_count:.1f}ê°œ")
        
        start_time = datetime.now()
        
        # ê° ë°°ì¹˜ë¥¼ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì²˜ë¦¬ (GPU ì „ìš©)
        from qwen_dedicated_worker import process_image_batch_worker
        
        loop = asyncio.get_event_loop()
        tasks = []
        
        for i, batch in enumerate(batches):
            # ë°°ì¹˜ ë°ì´í„° ì§ë ¬í™” (í”„ë¡œì„¸ìŠ¤ê°„ ì „ì†¡)
            batch_data = pickle.dumps(batch)
            device_id = i % gpu_count  # GPU í• ë‹¹
            
            # ê° í”„ë¡œì„¸ìŠ¤ê°€ ì „ìš© GPU ì‚¬ìš©
            task = loop.run_in_executor(
                self.process_pool, 
                process_image_batch_worker, 
                (device_id, batch_data)
            )
            tasks.append((task, device_id, len(batch)))
        
        print(f"âš¡ {len(tasks)}ê°œ GPU ì „ìš© í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘...")
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        all_results = {}
        page_counter = 1
        
        for task, device_id, batch_size in tasks:
            try:
                batch_results = await task
                
                for result in batch_results:
                    all_results[page_counter] = result
                    page_counter += 1
                
                print(f"âœ… GPU {device_id} ì „ìš© ì²˜ë¦¬ ì™„ë£Œ ({batch_size}ê°œ)")
                
            except Exception as e:
                print(f"âŒ GPU {device_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                for _ in range(batch_size):
                    all_results[page_counter] = f"<!-- GPU {device_id} ì²˜ë¦¬ ì‹¤íŒ¨ -->"
                    page_counter += 1
        
        # ê²°ê³¼ ì¡°í•©
        markdown_content = []
        for page_num in sorted(all_results.keys()):
            if page_num > 1:
                markdown_content.append("\n---\n")
            markdown_content.append(f"<!-- í˜ì´ì§€ {page_num} -->\n")
            markdown_content.append(all_results[page_num])
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        # ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘
        self._collect_performance_stats(total_time, total_images, gpu_count)
        
        print(f"\nğŸ“Š GPU ì „ìš© ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"  ğŸš€ ì²˜ë¦¬ëŸ‰: {total_images / total_time:.2f} í˜ì´ì§€/ì´ˆ")
        print(f"  ğŸ“ˆ GPUë³„ í‰ê· : {total_images / gpu_count / total_time:.2f} í˜ì´ì§€/ì´ˆ")
        print(f"  âš¡ ì˜¤í”„ë¡œë“œ ì˜¤ë²„í—¤ë“œ: ìµœì†Œí™”ë¨ (GPU ì „ìš©)")
        
        return "\n".join(markdown_content)
    
    def _collect_performance_stats(self, total_time: float, total_images: int, gpu_count: int):
        """ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘"""
        self.stats['total_images_processed'] = total_images
        self.stats['total_processing_time'] = total_time
        self.stats['processing_times'].append(total_time)
        self.stats['gpu_utilization_stats'] = {
            'gpu_count': gpu_count,
            'throughput_per_gpu': total_images / gpu_count / total_time,
            'total_throughput': total_images / total_time,
            'overhead_minimized': True
        }
    
    def get_dedicated_gpu_stats(self) -> Dict[str, Any]:
        """GPU ì „ìš© í†µê³„ ë°˜í™˜"""
        return {
            'mode': 'dedicated_gpu_optimized',
            'gpu_count': self.resource_manager.device_count,
            'offload_overhead': 'minimized',
            'worker_type': 'dedicated_single_gpu',
            'performance_stats': self.stats
        }
    
    def cleanup(self):
        """GPU ì „ìš© ì‹œìŠ¤í…œ ì •ë¦¬"""
        print("ğŸ§¹ GPU ì „ìš© ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        
        if self.process_pool:
            self.process_pool.shutdown(wait=True)
            self.process_pool = None
        
        self.workers_initialized = False
        
        # ëª¨ë“  GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.resource_manager.gpu_available:
            for i in range(self.resource_manager.device_count):
                try:
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                except:
                    pass
        
        gc.collect()
        print("âœ… GPU ì „ìš© ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")


async def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    client = DedicatedGPUQwenClient()
    
    print("ğŸ§ª GPU ì „ìš© í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸")
    if await client.initialize_dedicated_system():
        print("âœ… GPU ì „ìš© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ")
        
        stats = client.get_dedicated_gpu_stats()
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ì„¤ì •:")
        print(f"   GPU ìˆ˜: {stats['gpu_count']}ê°œ")
        print(f"   ëª¨ë“œ: {stats['mode']}")
        print(f"   ì˜¤í”„ë¡œë“œ: {stats['offload_overhead']}")
        
        client.cleanup()
    else:
        print("âŒ GPU ì „ìš© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(main())