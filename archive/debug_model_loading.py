"""
Qwen2.5-VL ëª¨ë¸ ë¡œë”© ë””ë²„ê¹…
ë‹¨ê³„ë³„ë¡œ ë¬¸ì œì ì„ í™•ì¸í•©ë‹ˆë‹¤
"""

import torch
import traceback
from pathlib import Path

def test_step1_tokenizer():
    """1ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë”©"""
    print("ğŸ” 1ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë”© í…ŒìŠ¤íŠ¸")
    try:
        from transformers import AutoTokenizer
        
        tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True
        )
        
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
        print(f"   ì–´íœ˜ í¬ê¸°: {len(tokenizer)}")
        return tokenizer
        
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None

def test_step2_processor():
    """2ë‹¨ê³„: í”„ë¡œì„¸ì„œ ë¡œë”©"""
    print("\nğŸ” 2ë‹¨ê³„: í”„ë¡œì„¸ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸")
    try:
        from transformers import AutoProcessor
        
        processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True
        )
        
        print("âœ… í”„ë¡œì„¸ì„œ ë¡œë“œ ì„±ê³µ")
        return processor
        
    except Exception as e:
        print(f"âŒ í”„ë¡œì„¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None

def test_step3_config():
    """3ë‹¨ê³„: ëª¨ë¸ ì„¤ì • í™•ì¸"""
    print("\nğŸ” 3ë‹¨ê³„: ëª¨ë¸ ì„¤ì • í™•ì¸")
    try:
        from transformers import AutoConfig
        
        config = AutoConfig.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True
        )
        
        print("âœ… ëª¨ë¸ ì„¤ì • ë¡œë“œ ì„±ê³µ")
        print(f"   ëª¨ë¸ íƒ€ì…: {config.model_type}")
        print(f"   ìˆ¨ê²¨ì§„ í¬ê¸°: {config.hidden_size}")
        print(f"   ì–´í…ì…˜ í—¤ë“œ: {config.num_attention_heads}")
        print(f"   ë ˆì´ì–´ ìˆ˜: {config.num_hidden_layers}")
        
        return config
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None

def test_step4_model_class():
    """4ë‹¨ê³„: ëª¨ë¸ í´ë˜ìŠ¤ í™•ì¸"""
    print("\nğŸ” 4ë‹¨ê³„: ëª¨ë¸ í´ë˜ìŠ¤ í™•ì¸")
    try:
        # ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
        from transformers import Qwen2VLForConditionalGeneration
        print("âœ… Qwen2VLForConditionalGeneration í´ë˜ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
        return Qwen2VLForConditionalGeneration
        
    except ImportError:
        try:
            # êµ¬ ë²„ì „ ë°©ì‹
            from transformers import AutoModelForCausalLM
            print("âš ï¸ AutoModelForCausalLMë¡œ ëŒ€ì²´ ì‹œë„")
            return AutoModelForCausalLM
        except Exception as e:
            print(f"âŒ ëª¨ë¸ í´ë˜ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

def test_step5_model_loading():
    """5ë‹¨ê³„: ì‹¤ì œ ëª¨ë¸ ë¡œë”©"""
    print("\nğŸ” 5ë‹¨ê³„: ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (CPU ì „ìš©)")
    try:
        from transformers import Qwen2VLForConditionalGeneration
        
        # CPUì—ì„œ ë¨¼ì € ì‹œë„
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32
        )
        
        print("âœ… ëª¨ë¸ CPU ë¡œë“œ ì„±ê³µ")
        print(f"   ëª¨ë¸ ë§¤ê°œë³€ìˆ˜: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ CPU ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False

def test_step6_gpu_loading():
    """6ë‹¨ê³„: GPU ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” 6ë‹¨ê³„: GPU ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    try:
        from transformers import Qwen2VLForConditionalGeneration
        
        print("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        torch.cuda.empty_cache()
        
        # GPUì—ì„œ ì‹œë„ (ë‹¨ì¼ GPU)
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="cuda:0",
            torch_dtype=torch.float16
        )
        
        print("âœ… ëª¨ë¸ GPU ë¡œë“œ ì„±ê³µ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ GPU ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False

def test_step7_auto_device():
    """7ë‹¨ê³„: Auto device í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” 7ë‹¨ê³„: Auto device ë§µí•‘ í…ŒìŠ¤íŠ¸")
    try:
        from transformers import Qwen2VLForConditionalGeneration
        
        print("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        torch.cuda.empty_cache()
        
        # Auto device mapping
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        print("âœ… ëª¨ë¸ Auto device ë¡œë“œ ì„±ê³µ")
        print(f"   ë””ë°”ì´ìŠ¤ ë§µ: {model.hf_device_map}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ Auto device ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False

def main():
    print("ğŸš€ Qwen2.5-VL ëª¨ë¸ ë¡œë”© ë””ë²„ê¹… ì‹œì‘")
    print("=" * 50)
    
    # ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸
    tokenizer = test_step1_tokenizer()
    processor = test_step2_processor()
    config = test_step3_config()
    model_class = test_step4_model_class()
    
    if tokenizer and processor and config and model_class:
        print("\nâœ… ê¸°ë³¸ êµ¬ì„±ìš”ì†Œ ëª¨ë‘ ë¡œë“œ ì„±ê³µ!")
        
        # ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        cpu_success = test_step5_model_loading()
        if cpu_success:
            gpu_success = test_step6_gpu_loading()
            if gpu_success:
                auto_success = test_step7_auto_device()
                
                if auto_success:
                    print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ! ëª¨ë¸ ë¡œë”© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                else:
                    print("\nâš ï¸ Auto device ë§µí•‘ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            else:
                print("\nâš ï¸ GPU ë¡œë”©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâŒ CPU ë¡œë”©ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ê¸°ë³¸ êµ¬ì„±ìš”ì†Œ ë¡œë”©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {allocated:.1f}GB í• ë‹¹ë¨, {cached:.1f}GB ìºì‹œë¨, {total:.1f}GB ì´ ë©”ëª¨ë¦¬")

if __name__ == "__main__":
    main()