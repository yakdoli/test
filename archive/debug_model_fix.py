"""
Qwen2.5-VL ëª¨ë¸ ë¡œë”© ìˆ˜ì •
ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
"""

import torch
import traceback

def test_correct_model_class():
    """ì˜¬ë°”ë¥¸ ëª¨ë¸ í´ë˜ìŠ¤ ì°¾ê¸°"""
    print("ğŸ” ì˜¬ë°”ë¥¸ ëª¨ë¸ í´ë˜ìŠ¤ í™•ì¸")
    
    # ê°€ëŠ¥í•œ í´ë˜ìŠ¤ë“¤ ì‹œë„
    classes_to_try = [
        ("Qwen2_5_VLForConditionalGeneration", "transformers"),
        ("Qwen2_5VLForConditionalGeneration", "transformers"),
        ("Qwen25VLForConditionalGeneration", "transformers"),
        ("AutoModelForVision2Seq", "transformers"),
        ("AutoModelForCausalLM", "transformers")
    ]
    
    for class_name, module_name in classes_to_try:
        try:
            module = __import__(module_name, fromlist=[class_name])
            model_class = getattr(module, class_name)
            print(f"âœ… {class_name} í´ë˜ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
            return model_class, class_name
        except AttributeError:
            print(f"âŒ {class_name} í´ë˜ìŠ¤ ì—†ìŒ")
        except Exception as e:
            print(f"âŒ {class_name} ì˜¤ë¥˜: {e}")
    
    return None, None

def test_auto_model():
    """AutoModelì„ ì‚¬ìš©í•œ ë¡œë”©"""
    print("\nğŸ” AutoModelì„ ì‚¬ìš©í•œ ë¡œë”© í…ŒìŠ¤íŠ¸")
    try:
        from transformers import AutoModel
        
        model = AutoModel.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32
        )
        
        print("âœ… AutoModel ë¡œë“œ ì„±ê³µ")
        print(f"   ëª¨ë¸ íƒ€ì…: {type(model)}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ AutoModel ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False

def test_alternative_loading():
    """ëŒ€ì•ˆì  ë¡œë”© ë°©ë²•"""
    print("\nğŸ” ëŒ€ì•ˆì  ë¡œë”© ë°©ë²• í…ŒìŠ¤íŠ¸")
    try:
        from transformers import AutoModelForCausalLM
        
        # trust_remote_code=Trueë¡œ ìë™ í´ë˜ìŠ¤ ë§¤í•‘
        model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32
        )
        
        print("âœ… AutoModelForCausalLM ë¡œë“œ ì„±ê³µ")
        print(f"   ì‹¤ì œ ëª¨ë¸ íƒ€ì…: {type(model)}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ AutoModelForCausalLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False

def test_with_revision():
    """íŠ¹ì • ë¦¬ë¹„ì „ ì‚¬ìš©"""
    print("\nğŸ” íŠ¹ì • ë¦¬ë¹„ì „ ì‚¬ìš© í…ŒìŠ¤íŠ¸")
    try:
        from transformers import AutoModelForCausalLM
        
        model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32,
            revision="main"
        )
        
        print("âœ… íŠ¹ì • ë¦¬ë¹„ì „ ë¡œë“œ ì„±ê³µ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ íŠ¹ì • ë¦¬ë¹„ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False

def test_compatible_model():
    """í˜¸í™˜ë˜ëŠ” ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” í˜¸í™˜ë˜ëŠ” ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    
    # ëŒ€ì•ˆ ëª¨ë¸ë“¤
    alternative_models = [
        "Qwen/Qwen2-VL-7B-Instruct",  # 2.0 ë²„ì „
        "Qwen/Qwen-VL-Chat",           # ì›ë˜ ë²„ì „
    ]
    
    for model_name in alternative_models:
        print(f"\nğŸ”„ {model_name} í…ŒìŠ¤íŠ¸:")
        try:
            from transformers import AutoModelForCausalLM
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                device_map="cpu",
                torch_dtype=torch.float32
            )
            
            print(f"âœ… {model_name} ë¡œë“œ ì„±ê³µ")
            print(f"   ëª¨ë¸ íƒ€ì…: {type(model)}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            torch.cuda.empty_cache()
            
            return True, model_name
            
        except Exception as e:
            print(f"âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return False, None

def main():
    print("ğŸš€ Qwen2.5-VL ëª¨ë¸ ë¡œë”© ìˆ˜ì • í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # 1. ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ ì°¾ê¸°
    model_class, class_name = test_correct_model_class()
    
    # 2. AutoModel ì‹œë„
    auto_success = test_auto_model()
    
    if not auto_success:
        # 3. AutoModelForCausalLM ì‹œë„
        causal_success = test_alternative_loading()
        
        if not causal_success:
            # 4. íŠ¹ì • ë¦¬ë¹„ì „ ì‹œë„
            revision_success = test_with_revision()
            
            if not revision_success:
                # 5. í˜¸í™˜ë˜ëŠ” ëª¨ë¸ ì‹œë„
                compatible_success, compatible_model = test_compatible_model()
                
                if compatible_success:
                    print(f"\nğŸ‰ í˜¸í™˜ë˜ëŠ” ëª¨ë¸ ë°œê²¬: {compatible_model}")
                else:
                    print(f"\nâŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
            else:
                print(f"\nğŸ‰ íŠ¹ì • ë¦¬ë¹„ì „ìœ¼ë¡œ ë¡œë“œ ì„±ê³µ!")
        else:
            print(f"\nğŸ‰ AutoModelForCausalLMìœ¼ë¡œ ë¡œë“œ ì„±ê³µ!")
    else:
        print(f"\nğŸ‰ AutoModelë¡œ ë¡œë“œ ì„±ê³µ!")

if __name__ == "__main__":
    main()