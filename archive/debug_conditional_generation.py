"""
ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ í´ë˜ìŠ¤ í™•ì¸
"""

import torch

def test_conditional_generation_models():
    """ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ í´ë˜ìŠ¤ë“¤ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ í´ë˜ìŠ¤ í™•ì¸")
    
    # ê°€ëŠ¥í•œ ì¡°ê±´ë¶€ ìƒì„± í´ë˜ìŠ¤ë“¤
    classes_to_try = [
        "Qwen2_5_VLForConditionalGeneration",
        "AutoModelForVision2Seq", 
        "AutoModelForSeq2SeqLM",
        "AutoModelForCausalLM"
    ]
    
    successful_classes = []
    
    for class_name in classes_to_try:
        print(f"\nğŸ”„ {class_name} í…ŒìŠ¤íŠ¸:")
        try:
            from transformers import AutoConfig
            import transformers
            
            # í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            model_class = getattr(transformers, class_name)
            
            # ì„¤ì •ìœ¼ë¡œ í˜¸í™˜ì„± í™•ì¸
            config = AutoConfig.from_pretrained(
                "Qwen/Qwen2.5-VL-7B-Instruct", 
                trust_remote_code=True
            )
            
            print(f"   ì„¤ì • í˜¸í™˜ì„±: âœ…")
            print(f"   ì•„í‚¤í…ì²˜: {config.architectures}")
            
            # ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹œë„ (CPU, ì‘ì€ ë©”ëª¨ë¦¬)
            model = model_class.from_pretrained(
                "Qwen/Qwen2.5-VL-7B-Instruct",
                trust_remote_code=True,
                device_map="cpu",
                torch_dtype=torch.float32
            )
            
            print(f"   ëª¨ë¸ ë¡œë“œ: âœ…")
            print(f"   ì‹¤ì œ íƒ€ì…: {type(model)}")
            
            # generate ë©”ì„œë“œ í™•ì¸
            if hasattr(model, 'generate'):
                print(f"   generate ë©”ì„œë“œ: âœ…")
                successful_classes.append((class_name, model_class))
            else:
                print(f"   generate ë©”ì„œë“œ: âŒ")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"   ì˜¤ë¥˜: {e}")
    
    return successful_classes

def test_direct_qwen25_class():
    """ì§ì ‘ Qwen2_5_VL í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ì§ì ‘ Qwen2_5_VLForConditionalGeneration í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸")
    
    try:
        # ì§ì ‘ import
        from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
        
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32
        )
        
        print("âœ… ì§ì ‘ Qwen2_5_VLForConditionalGeneration ë¡œë“œ ì„±ê³µ")
        print(f"   ëª¨ë¸ íƒ€ì…: {type(model)}")
        print(f"   generate ë©”ì„œë“œ: {'âœ…' if hasattr(model, 'generate') else 'âŒ'}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ ì§ì ‘ í´ë˜ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_gpu_loading():
    """GPU ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” GPU ë¡œë”© í…ŒìŠ¤íŠ¸")
    
    try:
        from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="cuda:0",
            torch_dtype=torch.float16
        )
        
        print("âœ… GPU ë¡œë”© ì„±ê³µ")
        print(f"   ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        print(f"   GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {allocated:.1f}GB")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ GPU ë¡œë”© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("ğŸš€ ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ í´ë˜ìŠ¤ í™•ì¸")
    print("=" * 50)
    
    # 1. ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ í´ë˜ìŠ¤ë“¤ í…ŒìŠ¤íŠ¸
    successful_classes = test_conditional_generation_models()
    
    if successful_classes:
        print(f"\nâœ… ì„±ê³µí•œ í´ë˜ìŠ¤ë“¤:")
        for class_name, model_class in successful_classes:
            print(f"   - {class_name}: {model_class}")
    
    # 2. ì§ì ‘ Qwen2.5 í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
    direct_success = test_direct_qwen25_class()
    
    # 3. GPU ë¡œë”© í…ŒìŠ¤íŠ¸
    if direct_success:
        gpu_success = test_gpu_loading()
        
        if gpu_success:
            print(f"\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ! GPU ë¡œë”© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            print(f"\nâš ï¸ CPUëŠ” ê°€ëŠ¥í•˜ì§€ë§Œ GPU ë¡œë”©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâŒ ì§ì ‘ í´ë˜ìŠ¤ ë¡œë”©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()