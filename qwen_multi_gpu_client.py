"""
Qwen2.5-VL-7B-Instruct ë‹¤ì¤‘ GPU ìµœì í™” í´ë¼ì´ì–¸íŠ¸
Flash Attention 2 + ì „ì²´ GPU ë¦¬ì†ŒìŠ¤ ìµœëŒ€ í™œìš©
"""

import asyncio
import torch
import gc
import os
import psutil
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
from transformers import AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from modules.utils.prompt_utils import build_syncfusion_prompt
from modules.utils.md_staging import save_page_markdown
import config


class MultiGPUResourceManager:
    """ë‹¤ì¤‘ GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ë° ìµœì í™”"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        self.device_properties = {}
        self.device_memory = {}
        
        if self.gpu_available:
            for i in range(self.device_count):
                props = torch.cuda.get_device_properties(i)
                self.device_properties[i] = {
                    'name': props.name,
                    'total_memory': props.total_memory,
                    'compute_capability': f"{props.major}.{props.minor}"
                }
                
    def get_optimal_multi_gpu_config(self) -> Dict[str, Any]:
        """ë‹¤ì¤‘ GPU ìµœì  ì„¤ì • ë°˜í™˜"""
        if not self.gpu_available or self.device_count < 2:
            return self._get_single_gpu_config()
        
        # ì „ì²´ GPU ë©”ëª¨ë¦¬ ê³„ì‚°
        total_gpu_memory = sum(props['total_memory'] for props in self.device_properties.values())
        total_gpu_memory_gb = total_gpu_memory / (1024**3)
        
        # ì‹œìŠ¤í…œ RAM í™•ì¸
        system_memory = psutil.virtual_memory()
        system_memory_gb = system_memory.total / (1024**3)
        
        print(f"ğŸ”§ ë‹¤ì¤‘ GPU ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:")
        print(f"   GPU ê°œìˆ˜: {self.device_count}")
        print(f"   ì´ GPU ë©”ëª¨ë¦¬: {total_gpu_memory_gb:.1f}GB")
        print(f"   ì‹œìŠ¤í…œ RAM: {system_memory_gb:.1f}GB ({system_memory.percent}% ì‚¬ìš©ì¤‘)")
        
        for i, props in self.device_properties.items():
            memory_gb = props['total_memory'] / (1024**3)
            print(f"   GPU {i}: {props['name']} ({memory_gb:.1f}GB)")
        
        # Qwen2.5-VL-7BëŠ” ì•½ 14GB í•„ìš” (FP16 ê¸°ì¤€)
        # ë‹¤ì¤‘ GPUë¡œ ë¶„ì‚° ë¡œë“œ
        if total_gpu_memory_gb >= 20:  # ì—¬ìœ ìˆëŠ” ë‹¤ì¤‘ GPU VRAM
            device_config = {
                "device_map": "auto",  # ìë™ ë¶„ì‚°
                "torch_dtype": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
                "max_memory": self._calculate_max_memory_per_gpu()
            }
            print("âœ… ë‹¤ì¤‘ GPU ìë™ ë¶„ì‚° ë¡œë“œ (ìµœê³  ì„±ëŠ¥)")
        elif total_gpu_memory_gb >= 16:  # ìµœì†Œ ë‹¤ì¤‘ GPU VRAM
            device_config = {
                "device_map": "auto",
                "torch_dtype": torch.float16,
                "load_in_8bit": True,  # 8ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
                "max_memory": self._calculate_max_memory_per_gpu()
            }
            print("âš¡ ë‹¤ì¤‘ GPU 8ë¹„íŠ¸ ì–‘ìí™” ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)")
        else:
            # ë‹¨ì¼ GPU í´ë°±
            return self._get_single_gpu_config()
        
        return device_config
    
    def _get_single_gpu_config(self) -> Dict[str, Any]:
        """ë‹¨ì¼ GPU ì„¤ì •"""
        if not self.gpu_available:
            return {"device_map": "cpu", "torch_dtype": torch.float32}
        
        gpu_memory_gb = self.device_properties[0]['total_memory'] / (1024**3)
        
        if gpu_memory_gb >= 16:
            return {
                "device_map": "cuda:0",
                "torch_dtype": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            }
        else:
            return {
                "device_map": "cuda:0",
                "torch_dtype": torch.float16,
                "load_in_8bit": True
            }
    
    def _calculate_max_memory_per_gpu(self) -> Dict[int, str]:
        """ê° GPUì˜ ìµœëŒ€ ì‚¬ìš© ë©”ëª¨ë¦¬ ê³„ì‚°"""
        max_memory = {}
        
        for i in range(self.device_count):
            # ê° GPU ë©”ëª¨ë¦¬ì˜ 90% ì‚¬ìš© (ì‹œìŠ¤í…œ ì—¬ìœ ë¶„ í™•ë³´)
            total_memory = self.device_properties[i]['total_memory']
            usable_memory = int(total_memory * 0.9)
            max_memory[i] = f"{usable_memory // (1024**3)}GB"
        
        return max_memory
    
    def cleanup_all_gpus(self):
        """ëª¨ë“  GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.gpu_available:
            for i in range(self.device_count):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
        gc.collect()
        
    def get_multi_gpu_memory_usage(self) -> Dict[str, Any]:
        """ë‹¤ì¤‘ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        usage = {"system_memory": psutil.virtual_memory().percent}
        
        if self.gpu_available:
            for i in range(self.device_count):
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                cached = torch.cuda.memory_reserved(i) / (1024**3)
                total = self.device_properties[i]['total_memory'] / (1024**3)
                
                usage[f'gpu_{i}'] = {
                    'allocated_gb': allocated,
                    'cached_gb': cached,
                    'total_gb': total,
                    'utilization': (allocated / total) * 100
                }
        
        return usage


class OptimizedMultiGPUQwenClient:
    """ìµœì í™”ëœ ë‹¤ì¤‘ GPU Qwen2.5-VL í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.resource_manager = MultiGPUResourceManager()
        self.device_config = None
        self.processing_lock = threading.Lock()  # ë™ì‹œ ì²˜ë¦¬ ì œì–´
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_processing_time': 0,
            'gpu_memory_peaks': {}
        }
        
    async def initialize_model(self) -> bool:
        """ë‹¤ì¤‘ GPU ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸš€ ë‹¤ì¤‘ GPU ìµœì í™” Qwen2.5-VL-7B-Instruct ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.resource_manager.cleanup_all_gpus()
            
            # ìµœì  ì„¤ì • ê²°ì •
            self.device_config = self.resource_manager.get_optimal_multi_gpu_config()
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
            os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
            os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                config.QWEN_MODEL_PATH,
                trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
            )
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            print("ğŸ¯ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
            self.processor = AutoProcessor.from_pretrained(
                config.QWEN_MODEL_PATH,
                trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
            )
            
            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ§  ë‹¤ì¤‘ GPU ëª¨ë¸ ë¡œë“œ ì¤‘...")
            load_kwargs = {
                "pretrained_model_name_or_path": config.QWEN_MODEL_PATH,
                "trust_remote_code": config.QWEN_TRUST_REMOTE_CODE,
                **self.device_config
            }
            
            # Flash Attention 2 ì„¤ì •
            if config.QWEN_USE_FLASH_ATTENTION and self.resource_manager.gpu_available:
                try:
                    load_kwargs["attn_implementation"] = "flash_attention_2"
                    print("âš¡ Flash Attention 2 í™œì„±í™” (ë©”ëª¨ë¦¬ ìµœì í™”)")
                except Exception:
                    print("âš ï¸ Flash Attention 2 ë¯¸ì§€ì› - ê¸°ë³¸ attention ì‚¬ìš©")
            
            # ë‹¤ì¤‘ GPU ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ê°€ ì„¤ì •
            if self.resource_manager.device_count > 1:
                load_kwargs["low_cpu_mem_usage"] = True
                print(f"ğŸ”— ë‹¤ì¤‘ GPU ë³‘ë ¬ ë¡œë“œ ì„¤ì • ({self.resource_manager.device_count}ê°œ GPU)")
            
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(**load_kwargs)
            
            # Evaluation ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
            # ëª¨ë¸ ë¶„ì‚° ì •ë³´ ì¶œë ¥
            if hasattr(self.model, 'hf_device_map'):
                print("ğŸ“Š ëª¨ë¸ ë””ë°”ì´ìŠ¤ ë¶„ì‚°:")
                for layer, device in self.model.hf_device_map.items():
                    print(f"   {layer}: {device}")
            
            # ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
            initial_memory = self.resource_manager.get_multi_gpu_memory_usage()
            print("ğŸ’¾ ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            for gpu_id, gpu_info in initial_memory.items():
                if gpu_id.startswith('gpu_'):
                    print(f"   {gpu_id}: {gpu_info['allocated_gb']:.1f}GB/{gpu_info['total_gb']:.1f}GB "
                          f"({gpu_info['utilization']:.1f}%)")
            
            print("âœ… ë‹¤ì¤‘ GPU ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë‹¤ì¤‘ GPU ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_syncfusion_prompt_legacy(self) -> str:
        """Syncfusion SDK ë§¤ë‰´ì–¼ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return """Convert this Syncfusion SDK documentation image to structured markdown format optimized for LLM fine-tuning and RAG applications.

CRITICAL REQUIREMENTS:

## Code Processing
- Extract ALL code snippets with proper language identification
- Preserve exact syntax, indentation, and formatting
- Use appropriate code blocks with language tags (```csharp, ```vb, ```xml, etc.)
- Maintain complete method signatures, parameter lists, and return types
- Include inline code elements using backticks for class names, properties, methods

## API Documentation Structure
- Identify and properly format: Classes, Namespaces, Methods, Properties, Events, Enums
- Use consistent heading hierarchy (# for main topics, ## for classes, ### for methods)
- Create clear parameter tables with: Name | Type | Description | Default Value
- Document return values with type and description
- Extract exception information if present

## Technical Content Enhancement
- Preserve all technical terminology exactly as written
- Maintain version-specific information and compatibility notes
- Include performance considerations and best practices
- Extract configuration settings and their valid values
- Document dependencies and required assemblies

## Structured Output Format
- Use descriptive headers that include class/namespace context
- Create linkable anchors for cross-references
- Format examples with clear "Example:" or "Usage:" headers
- Include "See Also" sections for related APIs
- Add metadata comments for categorization

## Content Completeness
- Extract ALL visible text without omission
- Preserve table structures with proper markdown formatting
- Maintain numbered/bulleted lists with correct nesting
- Include notes, warnings, and tips in appropriate callout format
- Capture image captions and figure references

## RAG Optimization
- Use semantic section breaks for better chunking
- Include contextual keywords for improved searchability
- Maintain hierarchical relationships between parent/child concepts
- Add implicit context where beneficial for standalone understanding

Focus on creating documentation that serves as high-quality training data for LLM fine-tuning while being immediately useful for RAG retrieval systems."""

    # NOTE: Deprecated local prompt function (replaced by modules.utils.prompt_utils.build_syncfusion_prompt)
    # def get_syncfusion_prompt(self, image_path: Path) -> str:
        """Syncfusion íŠ¹í™” í”„ë¡¬í”„íŠ¸ (ë™ì  ë©”íƒ€ë°ì´í„° í¬í•¨)
        - ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹/RAG ì¼ê´€ì„± ê°•í™”ë¥¼ ìœ„í•´ ë™ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì§€ì‹œë¬¸
        - ì»¨í…ìŠ¤íŠ¸/ë©”íƒ€ ì •ë³´ í‘œì¤€í™”, ì„¹ì…˜ ìŠ¤í‚¤ë§ˆ ê³ ì •, ì–¸ì–´/ì½”ë“œ/í‘œ/ë¦¬ìŠ¤íŠ¸/ë§í¬ ì²˜ë¦¬ ê·œì•½ ê°•í™”
        - OCR ì •ê·œí™” ê·œì¹™ í¬í•¨ (ë„ì–´ì“°ê¸°, í•˜ì´í”ˆ ì¤„ë°”ê¿ˆ, íŠ¹ìˆ˜ë¬¸ì í†µí•© ë“±)
        """
        import re
        from datetime import datetime

        file_name = image_path.name
        parent_dir = image_path.parent.name
        stem = image_path.stem
        m = re.search(r"page[_-]?(\d+)", stem, re.IGNORECASE)
        page_number = m.group(1) if m else "unknown"
        iso_ts = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

        return f"""
You are a meticulous technical documentation OCR and structuring agent specialized in Syncfusion SDK manuals.
Your task is to convert the given documentation image into HIGH-FIDELITY Markdown that is suitable for LLM fine-tuning datasets and RAG retrieval.

CONTEXT VALUES (use EXACTLY in the metadata header):
- source: image
- domain: syncfusion-sdk
- task: pdf-ocr-to-markdown
- language: auto (keep original; do not translate)
- source_filename: {file_name}
- document_name: {parent_dir}
- page_number: {page_number}
- page_id: {parent_dir}#page_{page_number}
- timestamp: {iso_ts}
- fidelity: lossless

GLOBAL OUTPUT CONTRACT (MUST FOLLOW EXACTLY):
- Top-level must start with an HTML comment metadata block:
  <!--
  source: image
  domain: syncfusion-sdk
  task: pdf-ocr-to-markdown
  language: auto (keep original; do not translate)
  source_filename: {file_name}
  document_name: {parent_dir}
  page_number: {page_number}
  page_id: {parent_dir}#page_{page_number}
  timestamp: {iso_ts}
  fidelity: lossless
  -->
- After metadata, output the structured content only in Markdown. No extra explanations.
- Do not invent content. If text is cropped/unclear, include "[unclear]" and keep position.
- Preserve all text as-is except for OCR normalization rules below.

OCR NORMALIZATION RULES:
- Merge hyphenated line breaks: "inter-
  face" -> "interface" when it's the same token.
- Normalize multiple spaces to single spaces, but preserve indentation inside code blocks.
- Preserve Unicode punctuation and math symbols as-is.
- Keep list numbering as shown (don't renumber).
- Keep casing exactly; do not title-case or sentence-case.

STRUCTURE SCHEMA (ENFORCE):
# {{Page/Main Title}}

## Overview
- 1-3 bullets summarizing the page scope using only visible text.

## Content
- Reconstruct hierarchy (H2/H3/H4) exactly as in image.
- Tables: use GitHub-flavored Markdown. Keep column order, headers, alignment if visible.
- Lists: preserve nesting and markers (-, *, 1.) as-is.
- Callouts: map to blockquotes with labels (Note:, Warning:, Tip:).
- Figures/Captions: include as "Figure: ..." lines when present.

## API Reference (if applicable)
- Namespace, Class, Members (Methods/Properties/Events/Enums) in subsections.
- Parameters table: Name | Type | Description | Default | Required
- Returns: Type + description.
- Exceptions: bullet list.

## Code Examples (multi-language supported)
- Extract ALL code exactly. Use fenced blocks with language: ```csharp, ```vb, ```xml, ```xaml, ```js, ```css, ```ts, ```python.
- Keep full signatures, imports/usings, comments, region markers.
- Inline code in text should be wrapped with backticks.

## Cross References
- Add See also: bullet list of explicit links/texts present on the page. Do not fabricate.

## RAG Annotations
- At the end, add an HTML comment with tags and keywords derived ONLY from visible content:
  <!-- tags: [product, module, control, api, version?] keywords: [k1, k2, ...] -->

ADDITIONAL RULES:
- Units, versions, file paths, and identifiers must be preserved exactly.
- Do not reflow long lines inside code blocks.
- Preserve table cell line breaks using <br> if present.
- For cross-page references without URLs, keep the exact anchor text.

Output now in the specified format.
"""

    async def convert_single_image_optimized(self, image_path: Path, page_num: int) -> Optional[str]:
        """ë‹¤ì¤‘ GPU ìµœì í™”ëœ ë‹¨ì¼ ì´ë¯¸ì§€ ë³€í™˜"""
        with self.processing_lock:  # ë™ì‹œ ì²˜ë¦¬ ì œì–´
            start_time = datetime.now()
            self.stats['total_requests'] += 1
            
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": str(image_path)},
                            {"type": "text", "text": build_syncfusion_prompt(image_path)}
                        ]
                    }
                ]
                
                # qwen-vl-utilsë¥¼ ì‚¬ìš©í•œ ë¹„ì „ ì •ë³´ ì²˜ë¦¬
                text = self.processor.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                
                image_inputs, video_inputs = process_vision_info(messages)
                
                # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
                inputs = self.processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt"
                )
                
                # GPUë¡œ ì´ë™ (ì²« ë²ˆì§¸ GPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©)
                if self.resource_manager.gpu_available:
                    device = next(self.model.parameters()).device
                    inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}
                
                # ìµœì í™”ëœ ìƒì„± ì„¤ì •
                # ê²°ì •ë¡ ì  ìƒì„± ì„¤ì •: ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ì¼ê´€ì„± ë³´ì¥
                generation_config = {
                    "max_new_tokens": 8192,
                    "do_sample": False,
                    "top_p": 0.9,
                    "use_cache": True,
                    "repetition_penalty": 1.05,
                    "pad_token_id": self.tokenizer.eos_token_id,
                }
                
                # Flash Attention ìµœì í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
                with torch.no_grad():
                    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ gradient checkpointing ë¹„í™œì„±í™” (ì¶”ë¡  ì‹œ)
                    if hasattr(self.model, 'gradient_checkpointing_enable'):
                        self.model.gradient_checkpointing_disable()
                    
                    generated_ids = self.model.generate(**inputs, **generation_config)
                    
                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                output_text = self.processor.batch_decode(
                    generated_ids_trimmed, 
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False
                )[0]
                
                processing_time = (datetime.now() - start_time).total_seconds()
                self.stats['successful_requests'] += 1
                self.stats['total_processing_time'] += processing_time
                
                # GPU ë©”ëª¨ë¦¬ í”¼í¬ ê¸°ë¡
                if self.resource_manager.gpu_available:
                    current_memory = self.resource_manager.get_multi_gpu_memory_usage()
                    for gpu_id, gpu_info in current_memory.items():
                        if gpu_id.startswith('gpu_'):
                            peak_key = f"{gpu_id}_peak"
                            current_usage = gpu_info['allocated_gb']
                            self.stats['gpu_memory_peaks'][peak_key] = max(
                                self.stats['gpu_memory_peaks'].get(peak_key, 0),
                                current_usage
                            )
                
                # ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.resource_manager.gpu_available:
                    torch.cuda.empty_cache()
                
                return output_text
                
            except Exception as e:
                self.stats['failed_requests'] += 1
                print(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ ({image_path.name}): {e}")
                return None
    
    async def convert_images_to_markdown_parallel_optimized(self, image_paths: List[Path]) -> str:
        """ë‹¤ì¤‘ GPU ìµœì í™”ëœ ë³‘ë ¬ ì´ë¯¸ì§€ ë³€í™˜"""
        if not self.model:
            if not await self.initialize_model():
                return "ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
        
        total_images = len(image_paths)
        # GPU ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•œ ìµœì  ì²­í¬ í¬ê¸°
        chunk_size = min(config.CHUNK_SIZE, 2 if self.resource_manager.device_count > 1 else 3)
        
        print(f"ğŸš€ ë‹¤ì¤‘ GPU ìµœì í™” ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {total_images}ê°œ ì´ë¯¸ì§€")
        print(f"ğŸ“¦ ì²­í¬ í¬ê¸°: {chunk_size} (ë‹¤ì¤‘ GPU ë©”ëª¨ë¦¬ ìµœì í™”)")
        print(f"âš¡ Flash Attention 2: {'í™œì„±í™”' if config.QWEN_USE_FLASH_ATTENTION else 'ë¹„í™œì„±í™”'}")
        
        results = {}
        failed_pages = []
        start_time = datetime.now()
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, total_images, chunk_size):
            chunk = image_paths[i:i + chunk_size]
            chunk_num = i // chunk_size + 1
            total_chunks = (total_images + chunk_size - 1) // chunk_size
            
            print(f"\nğŸ“¦ ì²­í¬ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ ì´ë¯¸ì§€)")
            
            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
            memory_before = self.resource_manager.get_multi_gpu_memory_usage()
            
            # ì²­í¬ ë‚´ ì´ë¯¸ì§€ë“¤ì„ ìˆœì°¨ ì²˜ë¦¬ (ì•ˆì •ì„± ìš°ì„ )
            for j, image_path in enumerate(chunk):
                page_num = i + j + 1
                print(f"   ğŸ”„ í˜ì´ì§€ {page_num}/{total_images} ë³€í™˜ ì¤‘...")
                
                markdown_text = await self.convert_single_image_optimized(image_path, page_num)
                
                if markdown_text and markdown_text.strip():
                    results[page_num] = markdown_text
                    print(f"   âœ… í˜ì´ì§€ {page_num} ì™„ë£Œ ({len(markdown_text)}ì)")
                else:
                    results[page_num] = f"<!-- í˜ì´ì§€ {page_num} ë³€í™˜ ì‹¤íŒ¨ -->"
                    failed_pages.append(page_num)
                    print(f"   âŒ í˜ì´ì§€ {page_num} ì‹¤íŒ¨")

                # MD ìŠ¤í…Œì´ì§• ì €ì¥ (í˜ì´ì§€ ë‹¨ìœ„)
                try:
                    if markdown_text and markdown_text.strip():
                        save_page_markdown(
                            image_path,
                            markdown_text,
                            mode="scale_out",
                            prompt=build_syncfusion_prompt(image_path),
                            extra_meta={
                                "client": "OptimizedMultiGPUQwenClient"
                            },
                        )
                except Exception as _e:
                    print(f"   âš ï¸ MD ìŠ¤í…Œì´ì§• ì €ì¥ ê²½ê³ : {str(_e)}")
            
            # ì²­í¬ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
            memory_after = self.resource_manager.get_multi_gpu_memory_usage()
            
            print(f"ğŸ“Š ì²­í¬ {chunk_num} GPU ë©”ëª¨ë¦¬:")
            for gpu_id in [k for k in memory_after.keys() if k.startswith('gpu_')]:
                before = memory_before.get(gpu_id, {}).get('allocated_gb', 0)
                after = memory_after[gpu_id]['allocated_gb']
                total = memory_after[gpu_id]['total_gb']
                print(f"   {gpu_id}: {after:.1f}GB/{total:.1f}GB (Î”{after-before:+.1f}GB)")
            
            # ì²­í¬ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if chunk_num < total_chunks:
                print(f"   ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
                self.resource_manager.cleanup_all_gpus()
                await asyncio.sleep(1)
        
        # ê²°ê³¼ ì¡°í•©
        markdown_content = []
        for page_num in sorted(results.keys()):
            if page_num > 1:
                markdown_content.append("\n---\n")
            markdown_content.append(f"<!-- í˜ì´ì§€ {page_num} -->\n")
            markdown_content.append(results[page_num])
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š ë‹¤ì¤‘ GPU ìµœì í™” ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        print(f"  ğŸ“¦ ì²˜ë¦¬ëœ ì²­í¬: {total_chunks}ê°œ")
        print(f"  ğŸ“ˆ ì²˜ë¦¬ëŸ‰: {len(image_paths) / total_time:.2f} í˜ì´ì§€/ì´ˆ")
        print(f"  âœ… ì„±ê³µ: {self.stats['successful_requests']}/{self.stats['total_requests']}")
        print(f"  âŒ ì‹¤íŒ¨: {self.stats['failed_requests']}/{self.stats['total_requests']}")
        
        if self.stats['successful_requests'] > 0:
            avg_time = self.stats['total_processing_time'] / self.stats['successful_requests']
            print(f"  âš¡ í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ/í˜ì´ì§€")
        
        # GPU ë©”ëª¨ë¦¬ í”¼í¬ ì¶œë ¥
        if self.stats['gpu_memory_peaks']:
            print(f"  ğŸ’¾ GPU ë©”ëª¨ë¦¬ í”¼í¬:")
            for gpu_peak, peak_gb in self.stats['gpu_memory_peaks'].items():
                print(f"    {gpu_peak}: {peak_gb:.1f}GB")
        
        if failed_pages:
            print(f"  âš ï¸ ì‹¤íŒ¨í•œ í˜ì´ì§€: {sorted(failed_pages)}")
        
        return "\n".join(markdown_content)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        stats.update({
            'mode': 'multi_gpu_optimized',
            'gpu_count': self.resource_manager.device_count,
            'flash_attention_enabled': config.QWEN_USE_FLASH_ATTENTION,
            'device_config': self.device_config
        })
        return stats
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.model:
            del self.model
            self.model = None
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        if self.processor:
            del self.processor
            self.processor = None
        
        self.resource_manager.cleanup_all_gpus()
        print("ğŸ—‘ï¸ ë‹¤ì¤‘ GPU ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


async def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    client = OptimizedMultiGPUQwenClient()
    
    print("ğŸ§ª ë‹¤ì¤‘ GPU ìµœì í™” í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸")
    if await client.initialize_model():
        print("âœ… ë‹¤ì¤‘ GPU ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
        
        stats = client.get_performance_stats()
        print(f"ğŸ“Š ì„¤ì •: {stats['mode']}, GPU: {stats['gpu_count']}ê°œ, Flash Attention: {stats['flash_attention_enabled']}")
        
        client.cleanup()
    else:
        print("âŒ ë‹¤ì¤‘ GPU ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(main())