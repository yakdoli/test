"""
Flash Attention 2 íš¨ê³¼ í…ŒìŠ¤íŠ¸
"""

import torch
import time
import psutil
from pathlib import Path
from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
from transformers import AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import config

class FlashAttentionTester:
    def __init__(self):
        self.device = "cuda:0"
        torch.cuda.set_device(0)
        
    def load_model_with_attention(self, use_flash_attention: bool):
        """ì§€ì •ëœ attention íƒ€ì…ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ"""
        print(f"{'Flash Attention 2' if use_flash_attention else 'Standard Attention'} ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        # í† í¬ë‚˜ì´ì €ì™€ í”„ë¡œì„¸ì„œ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        
        processor = AutoProcessor.from_pretrained(
            config.QWEN_MODEL_PATH,
            trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
        )
        
        # ëª¨ë¸ ë¡œë“œ ì„¤ì •
        load_kwargs = {
            "pretrained_model_name_or_path": config.QWEN_MODEL_PATH,
            "trust_remote_code": config.QWEN_TRUST_REMOTE_CODE,
            "device_map": self.device,
            "torch_dtype": torch.bfloat16,
            "low_cpu_mem_usage": True,
        }
        
        if use_flash_attention:
            load_kwargs["attn_implementation"] = "flash_attention_2"
        else:
            load_kwargs["attn_implementation"] = "eager"
        
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(**load_kwargs)
        model.eval()
        
        return model, tokenizer, processor
    
    def get_memory_stats(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        cached = torch.cuda.memory_reserved(0) / (1024**3)
        return {"allocated": allocated, "cached": cached}
    
    def test_single_image(self, model, tokenizer, processor, image_path: Path, use_flash: bool):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print(f"\\n{'=' * 50}")
        print(f"{'Flash Attention 2' if use_flash else 'Standard Attention'} í…ŒìŠ¤íŠ¸")
        print(f"{'=' * 50}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ëª¨ë¸ ë¡œë“œ í›„)
        memory_before = self.get_memory_stats()
        print(f"ëª¨ë¸ ë¡œë“œ í›„ GPU ë©”ëª¨ë¦¬: {memory_before['allocated']:.2f}GB (ìºì‹œ: {memory_before['cached']:.2f}GB)")
        
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": str(image_path)},
                {"type": "text", "text": "Convert this image to markdown format with proper structure."}
            ]
        }]
        
        # ì „ì²˜ë¦¬
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        image_inputs, video_inputs = process_vision_info(messages)
        
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(self.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
        
        # ì „ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬
        memory_after_preprocess = self.get_memory_stats()
        print(f"ì „ì²˜ë¦¬ í›„ GPU ë©”ëª¨ë¦¬: {memory_after_preprocess['allocated']:.2f}GB (ìºì‹œ: {memory_after_preprocess['cached']:.2f}GB)")
        
        # ìƒì„± ì„¤ì •
        generation_config = {
            "max_new_tokens": 1000,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì„
            "do_sample": False,
            "pad_token_id": tokenizer.eos_token_id,
            "use_cache": True,
        }
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        peak_memory = 0
        
        with torch.no_grad():
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§í•˜ë©´ì„œ ì¶”ë¡  ì‹¤í–‰
            for _ in range(1):  # warmup
                generated_ids = model.generate(**inputs, **generation_config)
                current_memory = self.get_memory_stats()['allocated']
                peak_memory = max(peak_memory, current_memory)
        
        end_time = time.time()
        inference_time = end_time - start_time
        
        # ì‘ë‹µ ìƒì„±
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        # ìµœì¢… ë©”ëª¨ë¦¬
        memory_final = self.get_memory_stats()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\\nğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
        print(f"  â±ï¸ ì¶”ë¡  ì‹œê°„: {inference_time:.2f}ì´ˆ")
        print(f"  ğŸ”¥ í”¼í¬ ë©”ëª¨ë¦¬: {peak_memory:.2f}GB")
        print(f"  ğŸ“ ìƒì„±ëœ í† í°: {len(generated_ids_trimmed[0])} í† í°")
        print(f"  ğŸ“„ ì¶œë ¥ ê¸¸ì´: {len(output_text)} ë¬¸ì")
        print(f"  ğŸ’¾ ìµœì¢… ë©”ëª¨ë¦¬: {memory_final['allocated']:.2f}GB (ìºì‹œ: {memory_final['cached']:.2f}GB)")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        memory_efficiency = (peak_memory - memory_before['allocated']) / memory_before['allocated'] * 100
        print(f"  ğŸ“ˆ ë©”ëª¨ë¦¬ ì¦ê°€: {memory_efficiency:.1f}%")
        
        return {
            "inference_time": inference_time,
            "peak_memory": peak_memory,
            "memory_efficiency": memory_efficiency,
            "output_length": len(output_text),
            "tokens_generated": len(generated_ids_trimmed[0])
        }
    
    def run_comparison_test(self, image_path: Path):
        """Flash Attention vs Standard Attention ë¹„êµ í…ŒìŠ¤íŠ¸"""
        print("ğŸš€ Flash Attention íš¨ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 80)
        
        results = {}
        
        # 1. Standard Attention í…ŒìŠ¤íŠ¸
        print("\\n1ï¸âƒ£ Standard Attention í…ŒìŠ¤íŠ¸ ì¤‘...")
        model_std, tokenizer_std, processor_std = self.load_model_with_attention(False)
        results['standard'] = self.test_single_image(model_std, tokenizer_std, processor_std, image_path, False)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model_std, tokenizer_std, processor_std
        torch.cuda.empty_cache()
        time.sleep(2)
        
        # 2. Flash Attention í…ŒìŠ¤íŠ¸
        print("\\n2ï¸âƒ£ Flash Attention 2 í…ŒìŠ¤íŠ¸ ì¤‘...")
        model_flash, tokenizer_flash, processor_flash = self.load_model_with_attention(True)
        results['flash'] = self.test_single_image(model_flash, tokenizer_flash, processor_flash, image_path, True)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model_flash, tokenizer_flash, processor_flash
        torch.cuda.empty_cache()
        
        # 3. ë¹„êµ ê²°ê³¼
        self.print_comparison_results(results)
        
        return results
    
    def print_comparison_results(self, results):
        """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        print("\\n" + "=" * 80)
        print("ğŸ† Flash Attention vs Standard Attention ë¹„êµ ê²°ê³¼")
        print("=" * 80)
        
        std = results['standard']
        flash = results['flash']
        
        # ì†ë„ ë¹„êµ
        speed_improvement = (std['inference_time'] - flash['inference_time']) / std['inference_time'] * 100
        print(f"\\nâš¡ ì¶”ë¡  ì†ë„:")
        print(f"  Standard:  {std['inference_time']:.2f}ì´ˆ")
        print(f"  Flash:     {flash['inference_time']:.2f}ì´ˆ")
        print(f"  ê°œì„ ë„:    {speed_improvement:+.1f}% {'âœ…' if speed_improvement > 0 else 'âŒ'}")
        
        # ë©”ëª¨ë¦¬ ë¹„êµ
        memory_improvement = (std['peak_memory'] - flash['peak_memory']) / std['peak_memory'] * 100
        print(f"\\nğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬:")
        print(f"  Standard:  {std['peak_memory']:.2f}GB")
        print(f"  Flash:     {flash['peak_memory']:.2f}GB")
        print(f"  ì ˆì•½ë„:    {memory_improvement:+.1f}% {'âœ…' if memory_improvement > 0 else 'âŒ'}")
        
        # í’ˆì§ˆ ë¹„êµ
        print(f"\\nğŸ“ ì¶œë ¥ í’ˆì§ˆ:")
        print(f"  Standard:  {std['output_length']} ë¬¸ì, {std['tokens_generated']} í† í°")
        print(f"  Flash:     {flash['output_length']} ë¬¸ì, {flash['tokens_generated']} í† í°")
        
        # ì „ì²´ í‰ê°€
        print(f"\\nğŸ¯ ì¢…í•© í‰ê°€:")
        if speed_improvement > 0 and memory_improvement > 0:
            print("  âœ… Flash Attention 2ê°€ ì†ë„ì™€ ë©”ëª¨ë¦¬ ëª¨ë‘ ê°œì„ ")
        elif speed_improvement > 0:
            print("  âš¡ Flash Attention 2ê°€ ì†ë„ ê°œì„  (ë©”ëª¨ë¦¬ëŠ” ë¹„ìŠ·)")
        elif memory_improvement > 0:
            print("  ğŸ’¾ Flash Attention 2ê°€ ë©”ëª¨ë¦¬ ì ˆì•½ (ì†ë„ëŠ” ë¹„ìŠ·)")
        else:
            print("  âš ï¸ Flash Attention 2 íš¨ê³¼ê°€ ì œí•œì ")

def main():
    # í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    image_path = None
    staging_dir = Path("staging/DICOM")
    if staging_dir.exists():
        image_files = list(staging_dir.glob("*.jpeg"))
        if image_files:
            image_path = image_files[0]  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì‚¬ìš©
    
    if not image_path or not image_path.exists():
        print("âŒ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   staging/DICOM/ ë””ë ‰í† ë¦¬ì— ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ–¼ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {image_path}")
    
    tester = FlashAttentionTester()
    results = tester.run_comparison_test(image_path)
    
    print(f"\\nâœ… Flash Attention í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()