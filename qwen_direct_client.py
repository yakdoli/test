"""
Qwen2.5-VL-7B-Instruct ì§ì ‘ ì‚¬ìš© í´ë¼ì´ì–¸íŠ¸
GPU/CPU/RAM ë¦¬ì†ŒìŠ¤ë¥¼ ìµœì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë¹„ë™ê¸° ì²˜ë¦¬
"""

import asyncio
import torch
import gc
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
import psutil
from tqdm.asyncio import tqdm

from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import config


class ResourceManager:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        
    def get_optimal_device_config(self) -> Dict[str, Any]:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        if not self.gpu_available:
            return {"device_map": "cpu", "torch_dtype": torch.float32}
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        total_gpu_memory = sum(torch.cuda.get_device_properties(i).total_memory 
                              for i in range(self.device_count))
        total_gpu_memory_gb = total_gpu_memory / (1024**3)
        
        # ì‹œìŠ¤í…œ RAM í™•ì¸
        system_memory = psutil.virtual_memory()
        system_memory_gb = system_memory.total / (1024**3)
        
        print(f"ğŸ”§ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´:")
        print(f"   GPU: {self.device_count}ê°œ, ì´ ë©”ëª¨ë¦¬: {total_gpu_memory_gb:.1f}GB")
        print(f"   RAM: {system_memory_gb:.1f}GB ({system_memory.percent}% ì‚¬ìš©ì¤‘)")
        
        # Qwen2.5-VL-7BëŠ” ì•½ 14GB VRAM í•„ìš” (FP16 ê¸°ì¤€)
        if total_gpu_memory_gb >= 16:  # ì—¬ìœ ìˆëŠ” VRAM
            device_config = {
                "device_map": "auto",
                "torch_dtype": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            }
            print("âœ… GPU ì „ìš© ë¡œë“œ (ìµœê³  ì„±ëŠ¥)")
        elif total_gpu_memory_gb >= 12:  # ìµœì†Œ VRAM
            device_config = {
                "device_map": "auto", 
                "torch_dtype": torch.float16,
                "load_in_8bit": True  # 8ë¹„íŠ¸ ì–‘ìí™”
            }
            print("âš¡ GPU 8ë¹„íŠ¸ ì–‘ìí™” ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)")
        elif system_memory_gb >= 32:  # CPU í´ë°±
            device_config = {
                "device_map": "cpu",
                "torch_dtype": torch.float32
            }
            print("ğŸ–¥ï¸ CPU ë¡œë“œ (GPU ë©”ëª¨ë¦¬ ë¶€ì¡±)")
        else:
            raise RuntimeError("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 32GB RAM ë˜ëŠ” 12GB VRAMì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return device_config
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.gpu_available:
            torch.cuda.empty_cache()
        gc.collect()
        
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        usage = {"system_memory": psutil.virtual_memory().percent}
        
        if self.gpu_available:
            for i in range(self.device_count):
                memory_info = torch.cuda.memory_stats(i)
                allocated = memory_info.get('allocated_bytes.all.current', 0) / (1024**3)
                cached = memory_info.get('reserved_bytes.all.current', 0) / (1024**3)
                usage[f'gpu_{i}_allocated'] = allocated
                usage[f'gpu_{i}_cached'] = cached
        
        return usage


class DirectQwenVLClient:
    """Qwen2.5-VL-7B-Instruct ì§ì ‘ ì‚¬ìš© í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.resource_manager = ResourceManager()
        self.device_config = None
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_processing_time': 0
        }
        
    async def initialize_model(self) -> bool:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸš€ Qwen2.5-VL-7B-Instruct ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            self.device_config = self.resource_manager.get_optimal_device_config()
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                config.QWEN_MODEL_PATH,
                trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
            )
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            print("ğŸ¯ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
            self.processor = AutoProcessor.from_pretrained(
                config.QWEN_MODEL_PATH,
                trust_remote_code=config.QWEN_TRUST_REMOTE_CODE
            )
            
            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ§  ëª¨ë¸ ë¡œë“œ ì¤‘...")
            load_kwargs = {
                "pretrained_model_name_or_path": config.QWEN_MODEL_PATH,
                "trust_remote_code": config.QWEN_TRUST_REMOTE_CODE,
                **self.device_config
            }
            
            # Flash Attention 2 ì§€ì› í™•ì¸
            if config.QWEN_USE_FLASH_ATTENTION and self.resource_manager.gpu_available:
                try:
                    load_kwargs["attn_implementation"] = "flash_attention_2"
                    print("âš¡ Flash Attention 2 í™œì„±í™”")
                except Exception:
                    print("âš ï¸ Flash Attention 2 ë¯¸ì§€ì› - ê¸°ë³¸ attention ì‚¬ìš©")
            
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(**load_kwargs)
            
            # Evaluation ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
            print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def get_syncfusion_prompt(self) -> str:
        """Syncfusion SDK ë§¤ë‰´ì–¼ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return """Convert this Syncfusion SDK documentation image to structured markdown format optimized for LLM fine-tuning and RAG applications.

CRITICAL REQUIREMENTS:

## Code Processing
- Extract ALL code snippets with proper language identification
- Preserve exact syntax, indentation, and formatting
- Use appropriate code blocks with language tags (```csharp, ```vb, ```xml, etc.)
- Maintain complete method signatures, parameter lists, and return types
- Include inline code elements using backticks for class names, properties, methods

## API Documentation Structure
- Identify and properly format: Classes, Namespaces, Methods, Properties, Events, Enums
- Use consistent heading hierarchy (# for main topics, ## for classes, ### for methods)
- Create clear parameter tables with: Name | Type | Description | Default Value
- Document return values with type and description
- Extract exception information if present

## Technical Content Enhancement
- Preserve all technical terminology exactly as written
- Maintain version-specific information and compatibility notes
- Include performance considerations and best practices
- Extract configuration settings and their valid values
- Document dependencies and required assemblies

## Structured Output Format
- Use descriptive headers that include class/namespace context
- Create linkable anchors for cross-references
- Format examples with clear "Example:" or "Usage:" headers
- Include "See Also" sections for related APIs
- Add metadata comments for categorization

## Content Completeness
- Extract ALL visible text without omission
- Preserve table structures with proper markdown formatting
- Maintain numbered/bulleted lists with correct nesting
- Include notes, warnings, and tips in appropriate callout format
- Capture image captions and figure references

## RAG Optimization
- Use semantic section breaks for better chunking
- Include contextual keywords for improved searchability
- Maintain hierarchical relationships between parent/child concepts
- Add implicit context where beneficial for standalone understanding

Focus on creating documentation that serves as high-quality training data for LLM fine-tuning while being immediately useful for RAG retrieval systems."""

    async def convert_image_to_markdown(self, image_path: Path) -> Optional[str]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
        if not self.model:
            if not await self.initialize_model():
                return None
        
        start_time = datetime.now()
        self.stats['total_requests'] += 1
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": str(image_path)},
                        {"type": "text", "text": self.get_syncfusion_prompt()}
                    ]
                }
            ]
            
            # qwen-vl-utilsë¥¼ ì‚¬ìš©í•œ ë¹„ì „ ì •ë³´ ì²˜ë¦¬
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            image_inputs, video_inputs = process_vision_info(messages)
            
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            
            # GPU ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.resource_manager.gpu_available:
                inputs = inputs.to(self.model.device)
            
            # ìƒì„± ì„¤ì •
            generation_config = {
                "max_new_tokens": 4000,
                "do_sample": False,
                "temperature": 0.1,
                "top_p": 0.9,
                "pad_token_id": self.tokenizer.eos_token_id
            }
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, **generation_config)
                
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            processing_time = (datetime.now() - start_time).total_seconds()
            self.stats['successful_requests'] += 1
            self.stats['total_processing_time'] += processing_time
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (GPU ë©”ëª¨ë¦¬ ê´€ë¦¬)
            if self.resource_manager.gpu_available:
                torch.cuda.empty_cache()
            
            return output_text
            
        except Exception as e:
            self.stats['failed_requests'] += 1
            print(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ ({image_path.name}): {e}")
            return None
    
    async def convert_images_to_markdown_parallel(self, image_paths: List[Path]) -> str:
        """ì´ë¯¸ì§€ë“¤ì„ ë³‘ë ¬ë¡œ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ (ì²­í¬ ë‹¨ìœ„)"""
        if not self.model:
            if not await self.initialize_model():
                return "ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
        
        total_images = len(image_paths)
        chunk_size = min(config.CHUNK_SIZE, 4)  # GPU ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ ì²­í¬ í¬ê¸° ì œí•œ
        
        print(f"ğŸš€ Direct Qwen2.5-VL ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {total_images}ê°œ ì´ë¯¸ì§€")
        print(f"ğŸ“¦ ì²­í¬ í¬ê¸°: {chunk_size} (GPU ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        results = {}
        failed_pages = []
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, total_images, chunk_size):
            chunk = image_paths[i:i + chunk_size]
            chunk_num = i // chunk_size + 1
            total_chunks = (total_images + chunk_size - 1) // chunk_size
            
            print(f"\nğŸ“¦ ì²­í¬ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ ì´ë¯¸ì§€)")
            
            # ì²­í¬ ë‚´ ì´ë¯¸ì§€ë“¤ì„ ìˆœì°¨ ì²˜ë¦¬ (GPU ë©”ëª¨ë¦¬ ì•ˆì •ì„±)
            for j, image_path in enumerate(chunk):
                page_num = i + j + 1
                print(f"   ğŸ”„ í˜ì´ì§€ {page_num}/{total_images} ë³€í™˜ ì¤‘...")
                
                markdown_text = await self.convert_image_to_markdown(image_path)
                
                if markdown_text and markdown_text.strip():
                    results[page_num] = markdown_text
                    print(f"   âœ… í˜ì´ì§€ {page_num} ì™„ë£Œ ({len(markdown_text)}ì)")
                else:
                    results[page_num] = f"<!-- í˜ì´ì§€ {page_num} ë³€í™˜ ì‹¤íŒ¨ -->"
                    failed_pages.append(page_num)
                    print(f"   âŒ í˜ì´ì§€ {page_num} ì‹¤íŒ¨")
                
                # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
                memory_usage = self.resource_manager.get_memory_usage()
                if memory_usage['system_memory'] > 85:
                    print(f"   âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_usage['system_memory']:.1f}%")
                    self.resource_manager.cleanup_memory()
            
            # ì²­í¬ ê°„ ì§§ì€ íœ´ì‹
            if chunk_num < total_chunks:
                await asyncio.sleep(1)
        
        # ê²°ê³¼ë¥¼ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ ìƒì„±
        markdown_content = []
        for page_num in sorted(results.keys()):
            if page_num > 1:
                markdown_content.append("\n---\n")
            markdown_content.append(f"<!-- í˜ì´ì§€ {page_num} -->\n")
            markdown_content.append(results[page_num])
        
        # í†µê³„ ì¶œë ¥
        success_count = len(results) - len(failed_pages)
        avg_time = (self.stats['total_processing_time'] / self.stats['successful_requests'] 
                   if self.stats['successful_requests'] > 0 else 0)
        
        print(f"\nğŸ“Š Direct Qwen2.5-VL ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  âœ… ì„±ê³µ: {success_count}/{total_images}")
        print(f"  âŒ ì‹¤íŒ¨: {len(failed_pages)}")
        print(f"  âš¡ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.1f}ì´ˆ/í˜ì´ì§€")
        
        if failed_pages:
            print(f"  âš ï¸ ì‹¤íŒ¨í•œ í˜ì´ì§€: {failed_pages}")
        
        return "\n".join(markdown_content)
    
    def cleanup(self):
        """ëª¨ë¸ ì •ë¦¬"""
        if self.model:
            del self.model
            self.model = None
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        if self.processor:
            del self.processor
            self.processor = None
        
        self.resource_manager.cleanup_memory()
        print("ğŸ—‘ï¸ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


async def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    client = DirectQwenVLClient()
    
    if await client.initialize_model():
        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
        memory_usage = client.resource_manager.get_memory_usage()
        print(f"ğŸ’¾ í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}")
    else:
        print("âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(main())